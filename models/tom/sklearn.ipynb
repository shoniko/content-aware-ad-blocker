{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REPO_ROOT = \"/usr/src/app\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model_type, train_size):\n",
    "    with open(\"%s/model-data/dataset_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def concat_models(model_names, train_size):\n",
    "    datasets = [load_model(name, train_size) for name in model_names]\n",
    "    \n",
    "    X_trains = [\n",
    "        normalize(sc.sparse.csr.csr_matrix(dataset[\"X_train\"]), norm='l2', axis=1)\n",
    "        for dataset in datasets\n",
    "    ]\n",
    "    X_tests = [\n",
    "        normalize(sc.sparse.csr.csr_matrix(dataset[\"X_test\"]), norm='l2', axis=1)\n",
    "        for dataset in datasets\n",
    "    ]\n",
    "       \n",
    "    concat_dataset = {\n",
    "        \"X_train\": sc.sparse.hstack(X_trains),\n",
    "        \"Y_train\": datasets[0][\"Y_train\"],\n",
    "        \"X_test\": sc.sparse.hstack(X_tests),\n",
    "        \"Y_test\": datasets[0][\"Y_test\"],\n",
    "        \"shas_test\": datasets[0][\"shas_test\"],\n",
    "    }\n",
    "    \n",
    "    print \"Datasets %s: %s = %s\" % (\n",
    "        model_names,\n",
    "        \" + \".join([str(np.shape(dataset[\"X_train\"])[1]) for dataset in datasets]),\n",
    "        np.shape(concat_dataset[\"X_train\"])[1])\n",
    "\n",
    "    print \"Labels equal: %s %s\" % (\n",
    "        [np.array_equal(datasets[0][\"Y_train\"], dataset[\"Y_train\"]) for dataset in datasets[1:]],\n",
    "        [np.array_equal(datasets[0][\"Y_test\"], dataset[\"Y_test\"]) for dataset in datasets[1:]])\n",
    "    \n",
    "    return concat_dataset\n",
    "\n",
    "def test_model(dataset, model_type, train_size, model, model_name, output_errors):\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "    test_pred = model.predict(dataset[\"X_test\"])\n",
    "    test_y = dataset[\"Y_test\"]\n",
    "    shas = dataset[\"shas_test\"]\n",
    "        \n",
    "    accuracy = (float(sum(test_y == test_pred))) / len(test_pred)\n",
    "    precision = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(max(1, sum(test_pred == 1)))\n",
    "    recall = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(sum(test_y == 1))\n",
    "    f1 = 2 * (precision * recall) / max(1, precision + recall)\n",
    "\n",
    "    print \"%10s %15s. Train set size %5d. %0.1f%% / %0.1f%% / %0.1f%% (%0.3f)\" % (\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size,\n",
    "        accuracy * 100,\n",
    "        precision * 100,\n",
    "        recall * 100,\n",
    "        f1)\n",
    "        \n",
    "    output_table.append([\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size, \n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "    ])\n",
    "    \n",
    "    if output_errors:\n",
    "        # Save 10 errors\n",
    "        error_shas = np.array(shas)[test_y != test_pred][0:50]\n",
    "        error_correct = np.array(test_y)[test_y != test_pred][0:50]\n",
    "\n",
    "        with open(\"%s/results/model_errors_%s_%s_%d.txt\" % (REPO_ROOT, model_type, model_name, train_size), \"w\") as fout:\n",
    "            for sha, correct in zip(error_shas, error_correct):\n",
    "                fout.write(\"#### %s FLAG: %s ####\\n\\n\" % (sha, \"Yes\" if correct > 0 else \"No\"))\n",
    "                with open(\"%s/scripts/%s.js\" % (REPO_ROOT, sha), \"r\") as fin:\n",
    "                    fout.write(fin.read())\n",
    "                fout.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var, input_size):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, input_size),\n",
    "                                     input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "        l_in_drop, num_units=20,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "        l_hid1_drop, num_units=10,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hid2_drop, num_units=2,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert np.shape(inputs)[0] == len(targets)\n",
    "    indices = np.arange(np.shape(inputs)[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, np.shape(inputs)[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        if isinstance(inputs[excerpt], np.ndarray):\n",
    "            i = inputs[excerpt]\n",
    "        else:\n",
    "            i = inputs[excerpt].toarray()\n",
    "        yield i, targets[excerpt]\n",
    "    \n",
    "def test_mlp(dataset, model_type, train_size):\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.lvector('targets')\n",
    "    # Create neural network model\n",
    "    network = build_mlp(input_var, np.shape(dataset[\"X_train\"])[1])\n",
    "    \n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.adam(loss, params)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "    \n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "    \n",
    "    if isinstance(dataset[\"X_train\"], np.ndarray):\n",
    "        X_train_flat = dataset[\"X_train\"]\n",
    "    else:\n",
    "        X_train_flat = dataset[\"X_train\"].tocsc()\n",
    "\n",
    "    if isinstance(dataset[\"X_test\"], np.ndarray):\n",
    "        X_test_flat = dataset[\"X_test\"]\n",
    "    else:\n",
    "        X_test_flat = dataset[\"X_test\"].tocsc()\n",
    "\n",
    "    best_accuracy = 0\n",
    "    bad_count = 0\n",
    "    batch_size = min(200, train_size/10)\n",
    "    for epoch in xrange(999):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train_flat, dataset[\"Y_train\"], batch_size, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_test_flat, dataset[\"Y_test\"], batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "        \n",
    "        current_accuracy = val_acc / val_batches\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} took {:.3f}s - accuracy {:.2f} %\".format(\n",
    "            epoch + 1, time.time() - start_time, current_accuracy * 100))\n",
    "        \n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            bad_count = 0\n",
    "        else:\n",
    "            bad_count += 1\n",
    "            if bad_count > 4:\n",
    "                break\n",
    "        \n",
    "    print \"%10s %15s. Train set size %5d. %0.1f%% / %0.1f%% / %0.1f%% (%0.3f)\" % (\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size,\n",
    "            current_accuracy * 100,\n",
    "            0,\n",
    "            0,\n",
    "            0)\n",
    "    output_table.append([\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size, \n",
    "            current_accuracy,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sizes: [300, 600, 1200, 2400, 4800, 9600, 19200]\n",
      "Test size: 3588\n"
     ]
    }
   ],
   "source": [
    "with open(\"%s/model-data/metadata.pickle\" % (REPO_ROOT,), \"r\") as f:\n",
    "    size_data = pickle.load(f)\n",
    "    \n",
    "TRAIN_SIZES = size_data[\"train_sizes\"]\n",
    "TEST_SIZE = size_data[\"test_size\"]\n",
    "\n",
    "print \"Training sizes: %s\" % TRAIN_SIZES\n",
    "print \"Test size: %d\" % TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RegEx             KNN. Train set size   300. 77.3% / 85.0% / 66.2% (0.745)\n",
      "     RegEx       Bernoulli. Train set size   300. 66.1% / 61.4% / 86.7% (0.719)\n",
      "     RegEx             SGD. Train set size   300. 80.8% / 83.6% / 76.8% (0.800)\n",
      "     RegEx    RandomForest. Train set size   300. 81.4% / 85.4% / 75.7% (0.802)\n",
      "     RegEx       LinearSVC. Train set size   300. 80.0% / 84.1% / 74.0% (0.787)\n",
      "   BiRegEx             KNN. Train set size   300. 77.8% / 86.9% / 65.4% (0.746)\n",
      "   BiRegEx       Bernoulli. Train set size   300. 63.4% / 58.8% / 89.2% (0.709)\n",
      "   BiRegEx             SGD. Train set size   300. 81.8% / 85.5% / 76.7% (0.808)\n",
      "   BiRegEx    RandomForest. Train set size   300. 79.9% / 83.1% / 75.0% (0.789)\n",
      "   BiRegEx       LinearSVC. Train set size   300. 81.4% / 86.3% / 74.7% (0.801)\n",
      "  TriRegEx             KNN. Train set size   300. 78.3% / 88.0% / 65.5% (0.751)\n",
      "  TriRegEx       Bernoulli. Train set size   300. 62.9% / 58.1% / 92.6% (0.714)\n",
      "  TriRegEx             SGD. Train set size   300. 81.9% / 85.6% / 76.7% (0.809)\n",
      "  TriRegEx    RandomForest. Train set size   300. 79.3% / 81.9% / 75.4% (0.785)\n",
      "  TriRegEx       LinearSVC. Train set size   300. 82.1% / 86.6% / 75.9% (0.809)\n",
      "       AST             KNN. Train set size   300. 77.1% / 84.1% / 67.0% (0.746)\n",
      "       AST       Bernoulli. Train set size   300. 64.9% / 59.9% / 89.9% (0.719)\n",
      "       AST             SGD. Train set size   300. 78.9% / 80.9% / 75.6% (0.782)\n",
      "       AST    RandomForest. Train set size   300. 79.8% / 82.4% / 75.8% (0.790)\n",
      "       AST       LinearSVC. Train set size   300. 79.0% / 83.1% / 72.7% (0.776)\n",
      "     BiAST             KNN. Train set size   300. 78.8% / 85.1% / 69.9% (0.768)\n",
      "     BiAST       Bernoulli. Train set size   300. 61.6% / 57.3% / 91.2% (0.704)\n",
      "     BiAST             SGD. Train set size   300. 80.2% / 83.9% / 74.7% (0.790)\n",
      "     BiAST    RandomForest. Train set size   300. 77.9% / 79.2% / 75.6% (0.774)\n",
      "     BiAST       LinearSVC. Train set size   300. 79.9% / 85.1% / 72.5% (0.783)\n",
      "    TriAST             KNN. Train set size   300. 79.2% / 86.7% / 69.0% (0.768)\n",
      "    TriAST       Bernoulli. Train set size   300. 60.6% / 56.3% / 93.9% (0.704)\n",
      "    TriAST             SGD. Train set size   300. 80.2% / 84.2% / 74.3% (0.789)\n",
      "    TriAST    RandomForest. Train set size   300. 76.4% / 76.0% / 77.2% (0.766)\n",
      "    TriAST       LinearSVC. Train set size   300. 80.1% / 85.5% / 72.5% (0.785)\n",
      "Random2Vec             KNN. Train set size   300. 74.1% / 86.1% / 57.4% (0.689)\n",
      "Random2Vec       Bernoulli. Train set size   300. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size   300. 68.3% / 84.1% / 45.1% (0.587)\n",
      "Random2Vec    RandomForest. Train set size   300. 71.0% / 71.8% / 69.3% (0.705)\n",
      "Random2Vec       LinearSVC. Train set size   300. 67.1% / 63.3% / 81.4% (0.712)\n",
      "  Word2Vec             KNN. Train set size   300. 75.6% / 84.7% / 62.4% (0.719)\n",
      "  Word2Vec       Bernoulli. Train set size   300. 70.0% / 77.1% / 57.0% (0.656)\n",
      "  Word2Vec             SGD. Train set size   300. 72.3% / 75.4% / 66.3% (0.705)\n",
      "  Word2Vec    RandomForest. Train set size   300. 79.4% / 83.0% / 73.9% (0.782)\n",
      "  Word2Vec       LinearSVC. Train set size   300. 75.6% / 74.7% / 77.3% (0.760)\n",
      "   AST2Vec             KNN. Train set size   300. 76.2% / 84.0% / 64.8% (0.732)\n",
      "   AST2Vec       Bernoulli. Train set size   300. 70.3% / 81.6% / 52.5% (0.639)\n",
      "   AST2Vec             SGD. Train set size   300. 74.8% / 73.4% / 77.8% (0.756)\n",
      "   AST2Vec    RandomForest. Train set size   300. 80.1% / 86.3% / 71.5% (0.782)\n",
      "   AST2Vec       LinearSVC. Train set size   300. 70.5% / 67.9% / 77.7% (0.725)\n",
      "     RegEx             KNN. Train set size   600. 79.8% / 87.0% / 70.2% (0.777)\n",
      "     RegEx       Bernoulli. Train set size   600. 65.0% / 60.3% / 87.5% (0.714)\n",
      "     RegEx             SGD. Train set size   600. 81.7% / 85.4% / 76.5% (0.807)\n",
      "     RegEx    RandomForest. Train set size   600. 82.1% / 86.3% / 76.3% (0.809)\n",
      "     RegEx       LinearSVC. Train set size   600. 81.4% / 85.5% / 75.6% (0.802)\n",
      "   BiRegEx             KNN. Train set size   600. 81.0% / 88.6% / 71.2% (0.789)\n",
      "   BiRegEx       Bernoulli. Train set size   600. 63.7% / 58.9% / 90.2% (0.713)\n",
      "   BiRegEx             SGD. Train set size   600. 82.6% / 86.0% / 77.9% (0.818)\n",
      "   BiRegEx    RandomForest. Train set size   600. 80.9% / 82.9% / 77.8% (0.803)\n",
      "   BiRegEx       LinearSVC. Train set size   600. 82.5% / 86.6% / 76.9% (0.815)\n",
      "  TriRegEx             KNN. Train set size   600. 81.8% / 90.6% / 70.9% (0.795)\n",
      "  TriRegEx       Bernoulli. Train set size   600. 62.6% / 57.9% / 92.6% (0.712)\n",
      "  TriRegEx             SGD. Train set size   600. 82.7% / 86.0% / 78.1% (0.819)\n",
      "  TriRegEx    RandomForest. Train set size   600. 79.5% / 79.2% / 80.2% (0.797)\n",
      "  TriRegEx       LinearSVC. Train set size   600. 82.7% / 87.2% / 76.8% (0.816)\n",
      "       AST             KNN. Train set size   600. 80.5% / 88.7% / 69.9% (0.782)\n",
      "       AST       Bernoulli. Train set size   600. 65.0% / 59.7% / 91.8% (0.724)\n",
      "       AST             SGD. Train set size   600. 81.9% / 85.5% / 76.9% (0.810)\n",
      "       AST    RandomForest. Train set size   600. 79.9% / 81.4% / 77.6% (0.795)\n",
      "       AST       LinearSVC. Train set size   600. 81.2% / 85.8% / 74.7% (0.799)\n",
      "     BiAST             KNN. Train set size   600. 81.5% / 91.0% / 70.0% (0.791)\n",
      "     BiAST       Bernoulli. Train set size   600. 62.3% / 57.6% / 93.1% (0.712)\n",
      "     BiAST             SGD. Train set size   600. 82.4% / 86.4% / 76.9% (0.814)\n",
      "     BiAST    RandomForest. Train set size   600. 78.9% / 79.0% / 78.7% (0.788)\n",
      "     BiAST       LinearSVC. Train set size   600. 82.2% / 87.1% / 75.7% (0.810)\n",
      "    TriAST             KNN. Train set size   600. 82.0% / 93.1% / 69.1% (0.793)\n",
      "    TriAST       Bernoulli. Train set size   600. 60.7% / 56.4% / 94.7% (0.707)\n",
      "    TriAST             SGD. Train set size   600. 82.6% / 87.1% / 76.5% (0.815)\n",
      "    TriAST    RandomForest. Train set size   600. 76.0% / 74.1% / 80.0% (0.769)\n",
      "    TriAST       LinearSVC. Train set size   600. 82.8% / 87.8% / 76.1% (0.815)\n",
      "Random2Vec             KNN. Train set size   600. 76.2% / 87.4% / 61.2% (0.720)\n",
      "Random2Vec       Bernoulli. Train set size   600. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size   600. 67.8% / 67.6% / 68.5% (0.681)\n",
      "Random2Vec    RandomForest. Train set size   600. 76.3% / 80.1% / 70.0% (0.747)\n",
      "Random2Vec       LinearSVC. Train set size   600. 68.7% / 79.2% / 50.7% (0.618)\n",
      "  Word2Vec             KNN. Train set size   600. 77.4% / 84.6% / 67.8% (0.753)\n",
      "  Word2Vec       Bernoulli. Train set size   600. 66.8% / 74.5% / 52.9% (0.619)\n",
      "  Word2Vec             SGD. Train set size   600. 78.8% / 78.4% / 80.4% (0.794)\n",
      "  Word2Vec    RandomForest. Train set size   600. 81.9% / 83.4% / 80.3% (0.818)\n",
      "  Word2Vec       LinearSVC. Train set size   600. 77.8% / 76.0% / 82.4% (0.791)\n",
      "   AST2Vec             KNN. Train set size   600. 78.6% / 82.9% / 72.9% (0.776)\n",
      "   AST2Vec       Bernoulli. Train set size   600. 69.5% / 82.1% / 51.3% (0.631)\n",
      "   AST2Vec             SGD. Train set size   600. 77.0% / 73.1% / 86.6% (0.792)\n",
      "   AST2Vec    RandomForest. Train set size   600. 80.9% / 80.4% / 82.6% (0.815)\n",
      "   AST2Vec       LinearSVC. Train set size   600. 74.1% / 71.9% / 80.5% (0.760)\n",
      "     RegEx             KNN. Train set size  1200. 82.0% / 86.8% / 75.5% (0.807)\n",
      "     RegEx       Bernoulli. Train set size  1200. 64.9% / 60.2% / 88.4% (0.716)\n",
      "     RegEx             SGD. Train set size  1200. 84.2% / 87.0% / 80.4% (0.836)\n",
      "     RegEx    RandomForest. Train set size  1200. 83.1% / 85.7% / 79.4% (0.824)\n",
      "     RegEx       LinearSVC. Train set size  1200. 84.7% / 87.9% / 80.5% (0.841)\n",
      "   BiRegEx             KNN. Train set size  1200. 79.6% / 79.1% / 80.5% (0.798)\n",
      "   BiRegEx       Bernoulli. Train set size  1200. 64.7% / 59.5% / 91.9% (0.722)\n",
      "   BiRegEx             SGD. Train set size  1200. 84.9% / 88.2% / 80.6% (0.842)\n",
      "   BiRegEx    RandomForest. Train set size  1200. 79.4% / 78.9% / 80.2% (0.795)\n",
      "   BiRegEx       LinearSVC. Train set size  1200. 85.7% / 89.6% / 80.8% (0.849)\n",
      "  TriRegEx             KNN. Train set size  1200. 76.8% / 73.3% / 84.3% (0.784)\n",
      "  TriRegEx       Bernoulli. Train set size  1200. 63.5% / 58.4% / 93.4% (0.719)\n",
      "  TriRegEx             SGD. Train set size  1200. 85.1% / 88.9% / 80.3% (0.844)\n",
      "  TriRegEx    RandomForest. Train set size  1200. 79.5% / 77.8% / 82.4% (0.801)\n",
      "  TriRegEx       LinearSVC. Train set size  1200. 85.9% / 90.1% / 80.5% (0.851)\n",
      "       AST             KNN. Train set size  1200. 83.1% / 91.9% / 72.6% (0.811)\n",
      "       AST       Bernoulli. Train set size  1200. 65.3% / 59.8% / 93.6% (0.730)\n",
      "       AST             SGD. Train set size  1200. 83.9% / 88.5% / 78.0% (0.829)\n",
      "       AST    RandomForest. Train set size  1200. 79.8% / 79.5% / 80.4% (0.800)\n",
      "       AST       LinearSVC. Train set size  1200. 84.4% / 89.5% / 77.8% (0.833)\n",
      "     BiAST             KNN. Train set size  1200. 83.8% / 93.2% / 72.9% (0.818)\n",
      "     BiAST       Bernoulli. Train set size  1200. 62.9% / 58.0% / 94.0% (0.717)\n",
      "     BiAST             SGD. Train set size  1200. 84.3% / 89.2% / 78.0% (0.833)\n",
      "     BiAST    RandomForest. Train set size  1200. 76.6% / 74.0% / 82.1% (0.778)\n",
      "     BiAST       LinearSVC. Train set size  1200. 84.7% / 90.1% / 77.9% (0.836)\n",
      "    TriAST             KNN. Train set size  1200. 84.3% / 94.4% / 73.0% (0.823)\n",
      "    TriAST       Bernoulli. Train set size  1200. 61.6% / 57.0% / 94.4% (0.711)\n",
      "    TriAST             SGD. Train set size  1200. 84.6% / 89.6% / 78.4% (0.836)\n",
      "    TriAST    RandomForest. Train set size  1200. 76.3% / 73.0% / 83.6% (0.779)\n",
      "    TriAST       LinearSVC. Train set size  1200. 85.0% / 90.2% / 78.6% (0.840)\n",
      "Random2Vec             KNN. Train set size  1200. 78.2% / 85.7% / 67.7% (0.756)\n",
      "Random2Vec       Bernoulli. Train set size  1200. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  1200. 70.4% / 70.8% / 69.5% (0.701)\n",
      "Random2Vec    RandomForest. Train set size  1200. 80.3% / 84.4% / 74.3% (0.790)\n",
      "Random2Vec       LinearSVC. Train set size  1200. 73.8% / 71.1% / 80.0% (0.753)\n",
      "  Word2Vec             KNN. Train set size  1200. 80.4% / 85.9% / 73.5% (0.792)\n",
      "  Word2Vec       Bernoulli. Train set size  1200. 66.5% / 74.4% / 51.8% (0.610)\n",
      "  Word2Vec             SGD. Train set size  1200. 80.7% / 80.3% / 82.1% (0.812)\n",
      "  Word2Vec    RandomForest. Train set size  1200. 83.2% / 86.0% / 80.0% (0.829)\n",
      "  Word2Vec       LinearSVC. Train set size  1200. 75.1% / 76.9% / 72.6% (0.747)\n",
      "   AST2Vec             KNN. Train set size  1200. 81.4% / 87.0% / 74.5% (0.802)\n",
      "   AST2Vec       Bernoulli. Train set size  1200. 67.0% / 80.8% / 45.9% (0.585)\n",
      "   AST2Vec             SGD. Train set size  1200. 78.1% / 75.3% / 84.8% (0.797)\n",
      "   AST2Vec    RandomForest. Train set size  1200. 81.8% / 83.0% / 80.7% (0.818)\n",
      "   AST2Vec       LinearSVC. Train set size  1200. 77.1% / 73.1% / 86.8% (0.794)\n",
      "     RegEx             KNN. Train set size  2400. 84.0% / 88.5% / 78.3% (0.831)\n",
      "     RegEx       Bernoulli. Train set size  2400. 65.9% / 60.9% / 89.2% (0.724)\n",
      "     RegEx             SGD. Train set size  2400. 85.5% / 87.8% / 82.4% (0.850)\n",
      "     RegEx    RandomForest. Train set size  2400. 83.6% / 84.4% / 82.4% (0.834)\n",
      "     RegEx       LinearSVC. Train set size  2400. 87.3% / 90.0% / 83.9% (0.868)\n",
      "   BiRegEx             KNN. Train set size  2400. 81.9% / 81.4% / 82.7% (0.821)\n",
      "   BiRegEx       Bernoulli. Train set size  2400. 64.8% / 59.6% / 92.1% (0.724)\n",
      "   BiRegEx             SGD. Train set size  2400. 86.9% / 89.6% / 83.5% (0.864)\n",
      "   BiRegEx    RandomForest. Train set size  2400. 80.2% / 78.0% / 84.0% (0.809)\n",
      "   BiRegEx       LinearSVC. Train set size  2400. 88.0% / 90.6% / 84.8% (0.876)\n",
      "  TriRegEx             KNN. Train set size  2400. 79.5% / 76.3% / 85.5% (0.807)\n",
      "  TriRegEx       Bernoulli. Train set size  2400. 63.9% / 58.7% / 93.3% (0.721)\n",
      "  TriRegEx             SGD. Train set size  2400. 87.2% / 90.4% / 83.3% (0.867)\n",
      "  TriRegEx    RandomForest. Train set size  2400. 77.5% / 74.3% / 83.9% (0.788)\n",
      "  TriRegEx       LinearSVC. Train set size  2400. 88.1% / 91.1% / 84.5% (0.877)\n",
      "       AST             KNN. Train set size  2400. 85.0% / 92.9% / 75.9% (0.835)\n",
      "       AST       Bernoulli. Train set size  2400. 65.6% / 59.9% / 95.0% (0.734)\n",
      "       AST             SGD. Train set size  2400. 84.7% / 89.0% / 79.3% (0.838)\n",
      "       AST    RandomForest. Train set size  2400. 77.3% / 74.6% / 82.7% (0.784)\n",
      "       AST       LinearSVC. Train set size  2400. 86.0% / 90.0% / 81.0% (0.853)\n",
      "     BiAST             KNN. Train set size  2400. 86.1% / 95.4% / 75.9% (0.845)\n",
      "     BiAST       Bernoulli. Train set size  2400. 63.4% / 58.2% / 95.1% (0.722)\n",
      "     BiAST             SGD. Train set size  2400. 85.9% / 90.0% / 80.8% (0.851)\n",
      "     BiAST    RandomForest. Train set size  2400. 75.9% / 72.2% / 84.3% (0.778)\n",
      "     BiAST       LinearSVC. Train set size  2400. 86.9% / 91.1% / 81.8% (0.862)\n",
      "    TriAST             KNN. Train set size  2400. 86.5% / 95.3% / 76.8% (0.850)\n",
      "    TriAST       Bernoulli. Train set size  2400. 62.2% / 57.4% / 94.1% (0.713)\n",
      "    TriAST             SGD. Train set size  2400. 86.5% / 90.8% / 81.2% (0.858)\n",
      "    TriAST    RandomForest. Train set size  2400. 76.8% / 73.6% / 83.6% (0.783)\n",
      "    TriAST       LinearSVC. Train set size  2400. 87.8% / 92.0% / 82.7% (0.871)\n",
      "Random2Vec             KNN. Train set size  2400. 80.2% / 87.2% / 70.8% (0.782)\n",
      "Random2Vec       Bernoulli. Train set size  2400. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  2400. 59.3% / 55.3% / 96.4% (0.703)\n",
      "Random2Vec    RandomForest. Train set size  2400. 81.9% / 85.5% / 76.8% (0.809)\n",
      "Random2Vec       LinearSVC. Train set size  2400. 74.9% / 73.1% / 78.9% (0.759)\n",
      "  Word2Vec             KNN. Train set size  2400. 83.5% / 90.0% / 75.1% (0.819)\n",
      "  Word2Vec       Bernoulli. Train set size  2400. 69.4% / 75.2% / 57.1% (0.649)\n",
      "  Word2Vec             SGD. Train set size  2400. 80.4% / 76.8% / 86.6% (0.814)\n",
      "  Word2Vec    RandomForest. Train set size  2400. 86.3% / 92.1% / 79.2% (0.852)\n",
      "  Word2Vec       LinearSVC. Train set size  2400. 79.1% / 76.6% / 83.3% (0.798)\n",
      "   AST2Vec             KNN. Train set size  2400. 82.5% / 90.0% / 72.8% (0.805)\n",
      "   AST2Vec       Bernoulli. Train set size  2400. 70.4% / 82.3% / 51.3% (0.632)\n",
      "   AST2Vec             SGD. Train set size  2400. 76.4% / 77.3% / 74.4% (0.758)\n",
      "   AST2Vec    RandomForest. Train set size  2400. 85.8% / 92.1% / 78.1% (0.845)\n",
      "   AST2Vec       LinearSVC. Train set size  2400. 79.8% / 79.3% / 80.2% (0.797)\n",
      "     RegEx             KNN. Train set size  4800. 86.0% / 89.6% / 81.4% (0.853)\n",
      "     RegEx       Bernoulli. Train set size  4800. 66.1% / 61.0% / 89.6% (0.726)\n",
      "     RegEx             SGD. Train set size  4800. 86.5% / 89.3% / 82.9% (0.860)\n",
      "     RegEx    RandomForest. Train set size  4800. 82.4% / 80.7% / 85.1% (0.828)\n",
      "     RegEx       LinearSVC. Train set size  4800. 89.2% / 91.4% / 86.4% (0.889)\n",
      "   BiRegEx             KNN. Train set size  4800. 83.9% / 82.8% / 85.6% (0.842)\n",
      "   BiRegEx       Bernoulli. Train set size  4800. 65.6% / 60.1% / 92.9% (0.730)\n",
      "   BiRegEx             SGD. Train set size  4800. 88.6% / 91.7% / 84.9% (0.882)\n",
      "   BiRegEx    RandomForest. Train set size  4800. 78.1% / 74.1% / 86.5% (0.798)\n",
      "   BiRegEx       LinearSVC. Train set size  4800. 90.2% / 92.5% / 87.6% (0.900)\n",
      "  TriRegEx             KNN. Train set size  4800. 81.5% / 78.2% / 87.2% (0.825)\n",
      "  TriRegEx       Bernoulli. Train set size  4800. 64.2% / 59.0% / 93.2% (0.723)\n",
      "  TriRegEx             SGD. Train set size  4800. 88.8% / 92.2% / 84.7% (0.883)\n",
      "  TriRegEx    RandomForest. Train set size  4800. 80.0% / 77.6% / 84.4% (0.809)\n",
      "  TriRegEx       LinearSVC. Train set size  4800. 90.6% / 92.9% / 87.8% (0.903)\n",
      "       AST             KNN. Train set size  4800. 87.7% / 94.0% / 80.5% (0.867)\n",
      "       AST       Bernoulli. Train set size  4800. 65.4% / 59.6% / 95.5% (0.734)\n",
      "       AST             SGD. Train set size  4800. 86.6% / 91.7% / 80.5% (0.858)\n",
      "       AST    RandomForest. Train set size  4800. 75.8% / 70.7% / 88.0% (0.784)\n",
      "       AST       LinearSVC. Train set size  4800. 88.4% / 91.4% / 84.8% (0.879)\n",
      "     BiAST             KNN. Train set size  4800. 87.9% / 94.8% / 80.2% (0.869)\n",
      "     BiAST       Bernoulli. Train set size  4800. 63.9% / 58.5% / 95.4% (0.725)\n",
      "     BiAST             SGD. Train set size  4800. 88.0% / 92.2% / 82.9% (0.873)\n",
      "     BiAST    RandomForest. Train set size  4800. 74.4% / 69.4% / 87.2% (0.773)\n",
      "     BiAST       LinearSVC. Train set size  4800. 89.0% / 91.8% / 85.6% (0.886)\n",
      "    TriAST             KNN. Train set size  4800. 88.2% / 95.1% / 80.5% (0.872)\n",
      "    TriAST       Bernoulli. Train set size  4800. 62.9% / 57.9% / 94.2% (0.717)\n",
      "    TriAST             SGD. Train set size  4800. 87.9% / 91.9% / 83.2% (0.873)\n",
      "    TriAST    RandomForest. Train set size  4800. 76.8% / 72.3% / 87.0% (0.789)\n",
      "    TriAST       LinearSVC. Train set size  4800. 89.3% / 92.0% / 86.2% (0.890)\n",
      "Random2Vec             KNN. Train set size  4800. 83.3% / 90.2% / 74.7% (0.817)\n",
      "Random2Vec       Bernoulli. Train set size  4800. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  4800. 71.7% / 72.0% / 70.9% (0.714)\n",
      "Random2Vec    RandomForest. Train set size  4800. 84.3% / 88.6% / 78.7% (0.833)\n",
      "Random2Vec       LinearSVC. Train set size  4800. 76.9% / 72.1% / 87.9% (0.792)\n",
      "  Word2Vec             KNN. Train set size  4800. 84.1% / 89.9% / 76.7% (0.828)\n",
      "  Word2Vec       Bernoulli. Train set size  4800. 67.4% / 73.0% / 55.1% (0.628)\n",
      "  Word2Vec             SGD. Train set size  4800. 79.8% / 73.6% / 92.9% (0.822)\n",
      "  Word2Vec    RandomForest. Train set size  4800. 87.7% / 92.9% / 81.5% (0.868)\n",
      "  Word2Vec       LinearSVC. Train set size  4800. 75.5% / 80.0% / 67.9% (0.735)\n",
      "   AST2Vec             KNN. Train set size  4800. 84.6% / 89.8% / 78.0% (0.835)\n",
      "   AST2Vec       Bernoulli. Train set size  4800. 68.1% / 79.8% / 48.6% (0.604)\n",
      "   AST2Vec             SGD. Train set size  4800. 78.9% / 73.6% / 90.1% (0.810)\n",
      "   AST2Vec    RandomForest. Train set size  4800. 87.7% / 92.9% / 81.5% (0.868)\n",
      "   AST2Vec       LinearSVC. Train set size  4800. 80.6% / 79.7% / 82.2% (0.809)\n",
      "     RegEx             KNN. Train set size  9600. 87.6% / 90.0% / 84.6% (0.872)\n",
      "     RegEx       Bernoulli. Train set size  9600. 66.8% / 61.4% / 90.5% (0.732)\n",
      "     RegEx             SGD. Train set size  9600. 86.5% / 89.2% / 83.0% (0.860)\n",
      "     RegEx    RandomForest. Train set size  9600. 80.0% / 76.3% / 87.1% (0.813)\n",
      "     RegEx       LinearSVC. Train set size  9600. 90.3% / 92.4% / 87.8% (0.901)\n",
      "   BiRegEx             KNN. Train set size  9600. 84.8% / 83.1% / 87.5% (0.852)\n",
      "   BiRegEx       Bernoulli. Train set size  9600. 66.1% / 60.4% / 93.4% (0.734)\n",
      "   BiRegEx             SGD. Train set size  9600. 88.7% / 91.4% / 85.4% (0.883)\n",
      "   BiRegEx    RandomForest. Train set size  9600. 77.3% / 72.4% / 88.4% (0.796)\n",
      "   BiRegEx       LinearSVC. Train set size  9600. 91.5% / 93.3% / 89.5% (0.914)\n",
      "  TriRegEx             KNN. Train set size  9600. 83.6% / 80.3% / 89.1% (0.845)\n",
      "  TriRegEx       Bernoulli. Train set size  9600. 64.7% / 59.3% / 93.1% (0.725)\n",
      "  TriRegEx             SGD. Train set size  9600. 89.0% / 92.1% / 85.4% (0.886)\n",
      "  TriRegEx    RandomForest. Train set size  9600. 79.6% / 76.0% / 86.6% (0.809)\n",
      "  TriRegEx       LinearSVC. Train set size  9600. 91.9% / 93.9% / 89.7% (0.917)\n",
      "       AST             KNN. Train set size  9600. 89.4% / 94.9% / 83.2% (0.887)\n",
      "       AST       Bernoulli. Train set size  9600. 66.0% / 60.0% / 95.8% (0.738)\n",
      "       AST             SGD. Train set size  9600. 87.4% / 92.2% / 81.8% (0.867)\n",
      "       AST    RandomForest. Train set size  9600. 73.2% / 67.9% / 88.3% (0.767)\n",
      "       AST       LinearSVC. Train set size  9600. 90.0% / 92.0% / 87.6% (0.898)\n",
      "     BiAST             KNN. Train set size  9600. 88.9% / 95.2% / 81.9% (0.881)\n",
      "     BiAST       Bernoulli. Train set size  9600. 64.1% / 58.7% / 94.8% (0.725)\n",
      "     BiAST             SGD. Train set size  9600. 88.1% / 92.8% / 82.6% (0.874)\n",
      "     BiAST    RandomForest. Train set size  9600. 73.9% / 68.7% / 87.7% (0.770)\n",
      "     BiAST       LinearSVC. Train set size  9600. 90.5% / 92.7% / 88.0% (0.903)\n",
      "    TriAST             KNN. Train set size  9600. 89.4% / 95.6% / 82.5% (0.886)\n",
      "    TriAST       Bernoulli. Train set size  9600. 63.1% / 58.1% / 94.0% (0.718)\n",
      "    TriAST             SGD. Train set size  9600. 88.2% / 92.3% / 83.3% (0.876)\n",
      "    TriAST    RandomForest. Train set size  9600. 77.8% / 73.5% / 86.8% (0.796)\n",
      "    TriAST       LinearSVC. Train set size  9600. 90.6% / 92.6% / 88.2% (0.904)\n",
      "Random2Vec             KNN. Train set size  9600. 84.6% / 90.2% / 77.8% (0.835)\n",
      "Random2Vec       Bernoulli. Train set size  9600. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  9600. 81.2% / 81.6% / 80.4% (0.810)\n",
      "Random2Vec    RandomForest. Train set size  9600. 86.1% / 90.8% / 80.4% (0.853)\n",
      "Random2Vec       LinearSVC. Train set size  9600. 77.4% / 72.0% / 89.6% (0.798)\n",
      "  Word2Vec             KNN. Train set size  9600. 87.2% / 92.1% / 81.7% (0.866)\n",
      "  Word2Vec       Bernoulli. Train set size  9600. 67.6% / 74.2% / 54.9% (0.631)\n",
      "  Word2Vec             SGD. Train set size  9600. 78.7% / 73.6% / 89.9% (0.809)\n",
      "  Word2Vec    RandomForest. Train set size  9600. 88.7% / 93.9% / 82.9% (0.881)\n",
      "  Word2Vec       LinearSVC. Train set size  9600. 81.6% / 77.6% / 89.3% (0.831)\n",
      "   AST2Vec             KNN. Train set size  9600. 86.6% / 90.4% / 82.2% (0.861)\n",
      "   AST2Vec       Bernoulli. Train set size  9600. 69.5% / 83.3% / 49.5% (0.621)\n",
      "   AST2Vec             SGD. Train set size  9600. 79.7% / 74.1% / 91.9% (0.820)\n",
      "   AST2Vec    RandomForest. Train set size  9600. 89.2% / 94.7% / 83.4% (0.887)\n",
      "   AST2Vec       LinearSVC. Train set size  9600. 81.9% / 78.8% / 87.8% (0.831)\n",
      "     RegEx             KNN. Train set size 19200. 88.9% / 91.2% / 86.2% (0.886)\n",
      "     RegEx       Bernoulli. Train set size 19200. 67.0% / 61.5% / 90.9% (0.734)\n",
      "     RegEx             SGD. Train set size 19200. 86.9% / 89.5% / 83.7% (0.865)\n",
      "     RegEx    RandomForest. Train set size 19200. 77.1% / 72.2% / 88.1% (0.794)\n",
      "     RegEx       LinearSVC. Train set size 19200. 91.6% / 93.5% / 89.5% (0.914)\n",
      "   BiRegEx             KNN. Train set size 19200. 86.4% / 84.8% / 88.6% (0.867)\n",
      "   BiRegEx       Bernoulli. Train set size 19200. 66.4% / 60.7% / 93.1% (0.735)\n",
      "   BiRegEx             SGD. Train set size 19200. 89.0% / 92.2% / 85.3% (0.886)\n",
      "   BiRegEx    RandomForest. Train set size 19200. 76.1% / 71.0% / 88.5% (0.788)\n",
      "   BiRegEx       LinearSVC. Train set size 19200. 92.4% / 93.9% / 90.7% (0.923)\n",
      "  TriRegEx             KNN. Train set size 19200. 85.4% / 82.7% / 89.5% (0.860)\n",
      "  TriRegEx       Bernoulli. Train set size 19200. 65.0% / 59.6% / 92.9% (0.726)\n",
      "  TriRegEx             SGD. Train set size 19200. 89.5% / 93.0% / 85.5% (0.891)\n",
      "  TriRegEx    RandomForest. Train set size 19200. 81.2% / 78.9% / 85.0% (0.819)\n",
      "  TriRegEx       LinearSVC. Train set size 19200. 92.5% / 93.8% / 91.0% (0.924)\n",
      "       AST             KNN. Train set size 19200. 90.4% / 95.9% / 84.3% (0.897)\n",
      "       AST       Bernoulli. Train set size 19200. 66.4% / 60.3% / 96.2% (0.741)\n",
      "       AST             SGD. Train set size 19200. 87.6% / 92.7% / 81.6% (0.868)\n",
      "       AST    RandomForest. Train set size 19200. 71.7% / 65.7% / 90.5% (0.761)\n",
      "       AST       LinearSVC. Train set size 19200. 91.3% / 93.3% / 89.1% (0.911)\n",
      "     BiAST             KNN. Train set size 19200. 90.2% / 95.7% / 84.3% (0.896)\n",
      "     BiAST       Bernoulli. Train set size 19200. 64.5% / 59.0% / 94.6% (0.727)\n",
      "     BiAST             SGD. Train set size 19200. 88.4% / 93.2% / 82.8% (0.877)\n",
      "     BiAST    RandomForest. Train set size 19200. 74.2% / 68.8% / 88.3% (0.774)\n",
      "     BiAST       LinearSVC. Train set size 19200. 91.5% / 93.5% / 89.1% (0.913)\n",
      "    TriAST             KNN. Train set size 19200. 87.9% / 96.9% / 78.3% (0.866)\n",
      "    TriAST       Bernoulli. Train set size 19200. 62.5% / 57.9% / 91.5% (0.709)\n",
      "    TriAST             SGD. Train set size 19200. 88.6% / 93.2% / 83.3% (0.880)\n",
      "    TriAST    RandomForest. Train set size 19200. 77.6% / 72.8% / 88.1% (0.797)\n",
      "    TriAST       LinearSVC. Train set size 19200. 91.8% / 94.1% / 89.2% (0.916)\n",
      "Random2Vec             KNN. Train set size 19200. 86.6% / 91.6% / 80.7% (0.858)\n",
      "Random2Vec       Bernoulli. Train set size 19200. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size 19200. 67.6% / 89.9% / 39.6% (0.550)\n",
      "Random2Vec    RandomForest. Train set size 19200. 87.7% / 92.9% / 81.5% (0.868)\n",
      "Random2Vec       LinearSVC. Train set size 19200. 79.4% / 78.4% / 81.2% (0.798)\n",
      "  Word2Vec             KNN. Train set size 19200. 88.1% / 92.6% / 82.8% (0.875)\n",
      "  Word2Vec       Bernoulli. Train set size 19200. 70.0% / 77.8% / 55.9% (0.650)\n",
      "  Word2Vec             SGD. Train set size 19200. 68.6% / 61.8% / 97.4% (0.756)\n",
      "  Word2Vec    RandomForest. Train set size 19200. 90.2% / 95.0% / 84.8% (0.896)\n",
      "  Word2Vec       LinearSVC. Train set size 19200. 66.9% / 60.6% / 97.0% (0.746)\n",
      "   AST2Vec             KNN. Train set size 19200. 88.5% / 92.6% / 83.7% (0.879)\n",
      "   AST2Vec       Bernoulli. Train set size 19200. 70.4% / 83.1% / 51.2% (0.634)\n",
      "   AST2Vec             SGD. Train set size 19200. 78.9% / 74.3% / 88.4% (0.807)\n",
      "   AST2Vec    RandomForest. Train set size 19200. 90.5% / 95.8% / 84.6% (0.899)\n",
      "   AST2Vec       LinearSVC. Train set size 19200. 62.3% / 57.4% / 95.8% (0.718)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    for model_type in [\"RegEx\", \"BiRegEx\", \"TriRegEx\", \"AST\", \"BiAST\", \"TriAST\", \"Random2Vec\", \"Word2Vec\", \"AST2Vec\"]:\n",
    "        dataset = load_model(model_type, train_size)\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   KNeighborsClassifier(2), \"KNN\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   BernoulliNB(), \"Bernoulli\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   LinearSVC(), \"LinearSVC\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "                \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/linear_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BiRegEx1K       LinearSVC. Train set size 19200. 92.2% / 94.4% / 89.7% (0.920)\n",
      " BiRegEx4K       LinearSVC. Train set size 19200. 92.3% / 94.1% / 90.1% (0.921)\n",
      "BiRegEx16K       LinearSVC. Train set size 19200. 92.6% / 94.2% / 90.9% (0.925)\n",
      "BiRegEx64K       LinearSVC. Train set size 19200. 92.3% / 93.6% / 90.7% (0.921)\n",
      "BiRegEx256K       LinearSVC. Train set size 19200. 92.5% / 94.0% / 90.8% (0.924)\n",
      " BiRegEx1M       LinearSVC. Train set size 19200. 92.4% / 93.9% / 90.7% (0.923)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "for model_type in [\"BiRegEx1K\", \"BiRegEx4K\", \"BiRegEx16K\", \"BiRegEx64K\", \"BiRegEx256K\", \"BiRegEx1M\"]:\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    test_model(dataset, model_type, train_size,\n",
    "               LinearSVC(), \"LinearSVC\",\n",
    "               train_size == TRAIN_SIZES[-1])\n",
    "                \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/truncated_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "for model_type in [\"RegEx\", \"TriRegEx\", \"AST\", \"TriAST\", \"AST2Vec\"]:\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    test_mlp(dataset, model_type, train_size)\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/mlp_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_table = []\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    for model_type in [\"Url3\", \"Url6\", \"Url12\"]:\n",
    "        dataset = load_model(model_type, train_size)\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   KNeighborsClassifier(2), \"KNN\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   BernoulliNB(), \"Bernoulli\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   LinearSVC(), \"LinearSVC\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/url_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "model_type = \"FileSize\"\n",
    "dataset = load_model(model_type, train_size)\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           KNeighborsClassifier(2), \"KNN\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           BernoulliNB(), \"Bernoulli\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           RandomForestClassifier(max_depth=15, n_estimators=100, max_features=15), \"RandomForest\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           LinearSVC(), \"LinearSVC\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/filesize_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ('BiRegEx', 'Url6', 'FileSize'): 500000 + 500000 + 15 = 1000015\n",
      "Labels equal: [True, True] [True, True]\n",
      "BiRegEx-Url6-FileSize    RandomForest. Train set size 19200. 82.9% / 93.6% / 70.6% (0.805)\n",
      "BiRegEx-Url6-FileSize       LinearSVC. Train set size 19200. 96.6% / 97.2% / 96.0% (0.966)\n",
      "Datasets ('BiRegEx1K', 'Url6', 'FileSize'): 118769 + 500000 + 15 = 618784\n",
      "Labels equal: [True, True] [True, True]\n",
      "BiRegEx1K-Url6-FileSize    RandomForest. Train set size 19200. 79.0% / 98.8% / 58.8% (0.737)\n",
      "BiRegEx1K-Url6-FileSize       LinearSVC. Train set size 19200. 96.6% / 97.6% / 95.6% (0.966)\n",
      "Datasets ('BiRegEx', 'Url6'): 500000 + 500000 = 1000000\n",
      "Labels equal: [True] [True]\n",
      "BiRegEx-Url6    RandomForest. Train set size 19200. 83.4% / 95.2% / 70.2% (0.808)\n",
      "BiRegEx-Url6       LinearSVC. Train set size 19200. 96.7% / 97.5% / 95.9% (0.967)\n",
      "Datasets ('BiRegEx1K', 'Url6'): 118769 + 500000 = 618769\n",
      "Labels equal: [True] [True]\n",
      "BiRegEx1K-Url6    RandomForest. Train set size 19200. 79.8% / 99.0% / 60.3% (0.749)\n",
      "BiRegEx1K-Url6       LinearSVC. Train set size 19200. 96.6% / 97.6% / 95.5% (0.965)\n",
      "Datasets ('BiRegEx', 'TriAST', 'Url6'): 500000 + 500000 + 500000 = 1500000\n",
      "Labels equal: [True, True] [True, True]\n",
      "BiRegEx-TriAST-Url6    RandomForest. Train set size 19200. 82.6% / 91.4% / 72.0% (0.805)\n",
      "BiRegEx-TriAST-Url6       LinearSVC. Train set size 19200. 96.2% / 97.3% / 95.0% (0.961)\n",
      "Datasets ('RegEx', 'Random2Vec'): 125020 + 1600 = 126620\n",
      "Labels equal: [True] [True]\n",
      "RegEx-Random2Vec    RandomForest. Train set size 19200. 83.6% / 92.0% / 73.6% (0.818)\n",
      "RegEx-Random2Vec       LinearSVC. Train set size 19200. 91.6% / 93.5% / 89.4% (0.914)\n",
      "Datasets ('RegEx', 'AST'): 125020 + 408414 = 533434\n",
      "Labels equal: [True] [True]\n",
      " RegEx-AST    RandomForest. Train set size 19200. 76.1% / 70.7% / 89.1% (0.789)\n",
      " RegEx-AST       LinearSVC. Train set size 19200. 92.5% / 93.8% / 91.1% (0.924)\n",
      "Datasets ('BiRegEx', 'Word2Vec'): 500000 + 1600 = 501600\n",
      "Labels equal: [True] [True]\n",
      "BiRegEx-Word2Vec    RandomForest. Train set size 19200. 82.2% / 85.3% / 77.9% (0.814)\n",
      "BiRegEx-Word2Vec       LinearSVC. Train set size 19200. 92.4% / 93.8% / 90.7% (0.922)\n",
      "Datasets ('BiRegEx', 'TriAST'): 500000 + 500000 = 1000000\n",
      "Labels equal: [True] [True]\n",
      "BiRegEx-TriAST    RandomForest. Train set size 19200. 77.5% / 72.6% / 88.3% (0.797)\n",
      "BiRegEx-TriAST       LinearSVC. Train set size 19200. 93.1% / 94.6% / 91.4% (0.930)\n",
      "Datasets ('Word2Vec', 'AST2Vec'): 1600 + 1600 = 3200\n",
      "Labels equal: [True] [True]\n",
      "Word2Vec-AST2Vec    RandomForest. Train set size 19200. 91.0% / 96.3% / 85.2% (0.904)\n",
      "Word2Vec-AST2Vec       LinearSVC. Train set size 19200. 86.9% / 91.4% / 81.4% (0.861)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "for model_names in [\n",
    "        (\"BiRegEx\", \"Url6\", \"FileSize\"),\n",
    "        (\"BiRegEx1K\", \"Url6\", \"FileSize\"),\n",
    "        (\"BiRegEx\", \"Url6\"),\n",
    "        (\"BiRegEx1K\", \"Url6\"),\n",
    "        (\"BiRegEx\", \"TriAST\", \"Url6\"),\n",
    "        (\"RegEx\", \"Random2Vec\"),\n",
    "        (\"RegEx\", \"AST\"),\n",
    "        (\"BiRegEx\", \"Word2Vec\"),\n",
    "        (\"BiRegEx\", \"TriAST\"),\n",
    "        (\"Word2Vec\", \"AST2Vec\") ]:\n",
    "    model_type = \"-\".join(model_names)\n",
    "    dataset = concat_models(model_names, train_size)\n",
    "\n",
    "    test_model(dataset, model_type, train_size,\n",
    "               RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\",\n",
    "               False)\n",
    "\n",
    "    test_model(dataset, model_type, train_size,\n",
    "               LinearSVC(), \"LinearSVC\",\n",
    "               False)\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/combined_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(X_test, Y_test, b, w):\n",
    "    nrows = np.shape(X_test)[0]\n",
    "    num_correct = 0\n",
    "    for row in xrange(nrows):\n",
    "        score = b + np.dot(X_test.getrow(row).toarray().flatten(), w)\n",
    "        pred = 1 if score > 0 else 0\n",
    "        actual = Y_test[row]\n",
    "        correct = (pred == actual)\n",
    "        if correct:\n",
    "            num_correct += 1\n",
    "        \n",
    "    return float(num_correct) / nrows\n",
    "\n",
    "def test_thresholds(dataset, b, w):\n",
    "    max_threshold = np.max(np.abs(w))\n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    if isinstance(dataset[\"X_test\"], np.ndarray):\n",
    "        X_test_flat = dataset[\"X_test\"]\n",
    "    else:\n",
    "        X_test_flat = dataset[\"X_test\"].tocsc()\n",
    "        \n",
    "    print \"Max weight: %f\" % max_threshold\n",
    "    for threshold in np.arange(0, max_threshold, max_threshold/100):\n",
    "        wprime = np.array(w)\n",
    "        wprime[np.abs(wprime) <= threshold] = 0\n",
    "        nonzero_count = np.sum(wprime != 0)\n",
    "        accuracy = calculate_accuracy(X_test_flat, dataset[\"Y_test\"], b, wprime)\n",
    "        best_accuracy = max(best_accuracy, accuracy)\n",
    "        print \"Accuracy at threshold %f (%d): %.1f%%\" % (threshold, nonzero_count, accuracy * 100)\n",
    "        if accuracy > 0.99 * best_accuracy:\n",
    "            best_threshold = threshold\n",
    "        else:\n",
    "            break\n",
    "    print \"Done. Best threshold: %f\" % best_threshold\n",
    "    return best_threshold\n",
    "\n",
    "def recover_input(tokenized_input, test_vector, vocab_info):\n",
    "    vocab_size = np.shape(vocab_info[\"idf\"])[0]\n",
    "    input_vector = np.zeros(vocab_size)\n",
    "\n",
    "    for tokens in tokenized_input:\n",
    "        #print \"%s -> %s\" % (tokens, vocab_info[\"vocab\"].get(tokens, \"-\"))\n",
    "        if tokens in vocab_info[\"vocab\"]:\n",
    "            index = vocab_info[\"vocab\"][tokens]\n",
    "            idf = vocab_info[\"idf\"][index]\n",
    "            input_vector[index] += idf\n",
    "\n",
    "    print \"norm: %s\" % np.linalg.norm(input_vector)\n",
    "    input_vector = input_vector / np.linalg.norm(input_vector)\n",
    "\n",
    "    print \"Test vector: %s\" % test_vector[(test_vector != 0) | (input_vector != 0)]\n",
    "    print \"Input vector: %s\" % input_vector[(test_vector != 0) | (input_vector != 0)]\n",
    "    \n",
    "    for idx in xrange(np.shape(test_vector)[0]):\n",
    "        if test_vector[idx] != 0 and input_vector[idx] == 0:\n",
    "            print \"Test %d %s = %f\" % (\n",
    "                idx, next(v for v, i in vocab_info[\"vocab\"].iteritems() if i == idx), test_vector[idx])\n",
    "        if test_vector[idx] == 0 and input_vector[idx] != 0:\n",
    "            print \"Input %d %s = %f\" % (\n",
    "                idx, next(v for v, i in vocab_info[\"vocab\"].iteritems() if i == idx), input_vector[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 266.414786012\n",
      "Test vector: [ 0.2211524   0.01987661  0.03141249 ...,  0.01292275  0.03264087\n",
      "  0.05653225]\n",
      "Input vector: [ 0.2211524   0.01987661  0.03141249 ...,  0.01292275  0.03264087\n",
      "  0.05653225]\n"
     ]
    }
   ],
   "source": [
    "def validate_url6():\n",
    "    model_type = \"Url6\"\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        vocab_info = pickle.load(f)\n",
    "\n",
    "    # Try to recover a test input\n",
    "    url = dataset[\"urls_test\"][0]\n",
    "    url_chars = util.ngramizer(util.tokenize_url, 6)(next(util.parse_url([{\"url\": url}])))\n",
    "    test_vector = dataset[\"X_test\"].getrow(0).toarray().flatten()\n",
    "    recover_input(url_chars, test_vector, vocab_info)\n",
    "\n",
    "validate_url6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 29.6930739147\n",
      "Test vector: [ 0.24964303  0.05723099  0.13799073  0.14139016  0.14156243  0.13050947\n",
      "  0.03799078  0.07120895  0.04096303  0.04144332  0.06007258  0.07801348\n",
      "  0.1397552   0.14156243  0.09203808  0.14151928  0.14156243  0.0825556\n",
      "  0.14147619  0.0858239   0.14020836  0.08382975  0.06164495  0.13644506\n",
      "  0.0827807   0.06107436  0.06706928  0.0481956   0.13316112  0.14049995\n",
      "  0.13955121  0.13967346  0.11388937  0.14156243  0.13959191  0.14156243\n",
      "  0.13765337  0.14156243  0.13370377  0.14143315  0.14156243  0.14156243\n",
      "  0.14156243  0.14156243  0.14100576  0.14151928  0.25863604  0.14156243\n",
      "  0.14156243  0.14104825  0.14139016  0.14156243  0.14156243  0.14139016\n",
      "  0.14143315  0.12887076  0.13054058  0.08011284  0.06076716  0.14156243\n",
      "  0.04019023  0.07205886  0.04207204  0.06811594]\n",
      "Input vector: [ 0.24964303  0.05723099  0.13799073  0.14139016  0.14156243  0.13050947\n",
      "  0.03799078  0.07120895  0.04096303  0.04144332  0.06007258  0.07801348\n",
      "  0.1397552   0.14156243  0.09203808  0.14151928  0.14156243  0.0825556\n",
      "  0.14147619  0.0858239   0.14020836  0.08382975  0.06164495  0.13644506\n",
      "  0.0827807   0.06107436  0.06706928  0.0481956   0.13316112  0.14049995\n",
      "  0.13955121  0.13967346  0.11388937  0.14156243  0.13959191  0.14156243\n",
      "  0.13765337  0.14156243  0.13370377  0.14143315  0.14156243  0.14156243\n",
      "  0.14156243  0.14156243  0.14100576  0.14151928  0.25863604  0.14156243\n",
      "  0.14156243  0.14104825  0.14139016  0.14156243  0.14156243  0.14139016\n",
      "  0.14143315  0.12887076  0.13054058  0.08011284  0.06076716  0.14156243\n",
      "  0.04019023  0.07205886  0.04207204  0.06811594]\n"
     ]
    }
   ],
   "source": [
    "def validate_bigregex():\n",
    "    model_type = \"BiRegEx1K\"\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        vocab_info = pickle.load(f)\n",
    "\n",
    "    # Try to recover a test input\n",
    "    sha = dataset[\"shas_test\"][0]\n",
    "    tokens = util.ngramizer(util.tokenize_js, 2)(next(util.parse_js([{\"sha\": sha}])))\n",
    "    test_vector = dataset[\"X_test\"].getrow(0).toarray().flatten()\n",
    "    recover_input(tokens, test_vector, vocab_info)\n",
    "\n",
    "validate_bigregex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max weight: 3.980342\n",
      "Accuracy at threshold 0.000000 (416203): 96.5%\n",
      "Accuracy at threshold 0.039803 (127899): 96.5%\n",
      "Accuracy at threshold 0.079607 (58556): 96.5%\n",
      "Accuracy at threshold 0.119410 (30121): 96.3%\n",
      "Accuracy at threshold 0.159214 (16497): 96.0%\n",
      "Accuracy at threshold 0.199017 (9596): 95.5%\n",
      "Done. Best threshold: 0.159214\n",
      "16497 indices\n"
     ]
    }
   ],
   "source": [
    "def create_url_model():\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "    dataset = load_model(\"Url6\", train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, \"Url6\", train_size), \"r\") as f:\n",
    "        url_vocab_info = pickle.load(f)\n",
    "\n",
    "    model = LinearSVC()\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "\n",
    "    b = model.intercept_[0]\n",
    "    w = model.coef_.flatten()\n",
    "\n",
    "    best_threshold = test_thresholds(dataset, b, w)\n",
    "\n",
    "    # Reduce vocabulary using the threshold\n",
    "    vocab_indices = np.nonzero(np.abs(w) > best_threshold)[0].tolist()\n",
    "\n",
    "    index_map = {\n",
    "        vocab_indices[idx]: idx\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "    }\n",
    "\n",
    "    print \"%d indices\" % len(vocab_indices)\n",
    "\n",
    "    url_model = {\n",
    "        \"vocab\": {\n",
    "            key: index_map[index]\n",
    "            for key, index in url_vocab_info[\"vocab\"].iteritems()\n",
    "            if index in index_map\n",
    "        },\n",
    "        \"idf\": [url_vocab_info[\"idf\"][idx] for idx in vocab_indices],\n",
    "        \"w\": [w[idx] for idx in vocab_indices],\n",
    "        \"b\": b,\n",
    "    }\n",
    "\n",
    "    with open(\"%s/model-data/url_model.js\" % REPO_ROOT, \"w\") as f:\n",
    "        f.write(\"const URLMODEL = %s;\\nfunction getUrlModel() { return URLMODEL; }\" % json.dumps(url_model))\n",
    "    \n",
    "create_url_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ['Url6', 'BiRegEx1K', 'FileSize']: 500000 + 118769 + 15 = 618784\n",
      "Labels equal: [True, True] [True, True]\n",
      "Max weight: 3.071873\n",
      "Accuracy at threshold 0.000000 (526858): 96.6%\n",
      "Accuracy at threshold 0.030719 (146013): 96.6%\n",
      "Accuracy at threshold 0.061437 (67920): 96.6%\n",
      "Accuracy at threshold 0.092156 (35719): 96.5%\n",
      "Accuracy at threshold 0.122875 (20571): 96.2%\n",
      "Accuracy at threshold 0.153594 (12673): 95.7%\n",
      "Accuracy at threshold 0.184312 (8099): 95.2%\n",
      "Done. Best threshold: 0.153594\n",
      "12673 indices (7892 url; 4771 script; 10 size)\n"
     ]
    }
   ],
   "source": [
    "def create_final_model():\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "\n",
    "    dataset = concat_models([\"Url6\", \"BiRegEx1K\", \"FileSize\"], train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, \"Url6\", train_size), \"r\") as f:\n",
    "        url_vocab_info = pickle.load(f)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, \"BiRegEx1K\", train_size), \"r\") as f:\n",
    "        script_vocab_info = pickle.load(f)\n",
    "\n",
    "    size_vocab = {\n",
    "        2**(n+5): n for n in xrange(0, 15)\n",
    "    }\n",
    "    \n",
    "    model = LinearSVC()\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "\n",
    "    b = model.intercept_[0]\n",
    "    w = model.coef_.flatten()\n",
    "\n",
    "    best_threshold = test_thresholds(dataset, b, w)\n",
    "\n",
    "    # Reduce vocabulary using the threshold\n",
    "    vocab_indices = np.nonzero(np.abs(w) > best_threshold)[0].tolist()\n",
    "\n",
    "    url_vocab_size = np.shape(url_vocab_info[\"idf\"])[0]\n",
    "    script_vocab_size = np.shape(script_vocab_info[\"idf\"])[0]\n",
    "\n",
    "    url_index_map = {\n",
    "        vocab_indices[idx]: idx\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "        if vocab_indices[idx] < url_vocab_size\n",
    "    }\n",
    "\n",
    "    url_new_vocab_size = len(url_index_map)\n",
    "    \n",
    "    script_index_map = {\n",
    "        (vocab_indices[idx] - url_vocab_size): idx - url_new_vocab_size\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "        if vocab_indices[idx] >= url_vocab_size and vocab_indices[idx] < url_vocab_size + script_vocab_size\n",
    "    }\n",
    "    \n",
    "    script_new_vocab_size = len(script_index_map)\n",
    "    \n",
    "    size_index_map = {\n",
    "        (vocab_indices[idx] - url_vocab_size - script_vocab_size): idx - url_new_vocab_size - script_new_vocab_size\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "        if vocab_indices[idx] >= url_vocab_size + script_vocab_size\n",
    "    }\n",
    "\n",
    "    print \"%d indices (%d url; %d script; %d size)\" % (\n",
    "        len(vocab_indices), len(url_index_map), len(script_index_map), len(size_index_map))\n",
    "\n",
    "    final_model = {\n",
    "        # Just the vocab & weights for URL features\n",
    "        \"url\": {\n",
    "            \"vocab\": {\n",
    "                key: url_index_map[index]\n",
    "                for key, index in url_vocab_info[\"vocab\"].iteritems()\n",
    "                if index in url_index_map\n",
    "            },\n",
    "            \"idf\": [url_vocab_info[\"idf\"][idx] for idx in vocab_indices[:url_new_vocab_size]],\n",
    "            \"w\": [w[idx] for idx in vocab_indices[:url_new_vocab_size]],\n",
    "        },\n",
    "        # Just the vocab & weights for the script features\n",
    "        \"script\": {\n",
    "             \"vocab\": {\n",
    "                key: script_index_map[index]\n",
    "                for key, index in script_vocab_info[\"vocab\"].iteritems()\n",
    "                if index in script_index_map\n",
    "            },\n",
    "            \"idf\": [script_vocab_info[\"idf\"][idx - url_vocab_size] for idx in vocab_indices[url_new_vocab_size:url_new_vocab_size + script_new_vocab_size]],\n",
    "            \"w\": [w[idx] for idx in vocab_indices[url_new_vocab_size:url_new_vocab_size + script_new_vocab_size]],\n",
    "        },\n",
    "        # Just the indices & weights for the file size features\n",
    "        \"size\": {\n",
    "            \"vocab\": {\n",
    "                key: size_index_map[index]\n",
    "                for key, index in size_vocab.iteritems()\n",
    "                if index in size_index_map\n",
    "            },\n",
    "            \"idf\": [1 for idx in vocab_indices[url_new_vocab_size + script_new_vocab_size:]],\n",
    "            \"w\": [w[idx] for idx in vocab_indices[url_new_vocab_size + script_new_vocab_size:]],\n",
    "        },\n",
    "        # The SVM intercept\n",
    "        \"b\": b,\n",
    "    }\n",
    "\n",
    "    with open(\"%s/model-data/final_model.js\" % REPO_ROOT, \"w\") as f:\n",
    "        f.write(\"const COMBOMODEL = %s;\\nfunction getCombinedModel() { return COMBOMODEL; }\" % json.dumps(final_model))\n",
    "    \n",
    "create_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
