{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REPO_ROOT = \"/usr/src/app\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "from sklearn.svm import *\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(model_type, train_size):\n",
    "    with open(\"%s/model-data/dataset_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def concat_models(model_names, train_size):\n",
    "    datasets = [load_model(name, train_size) for name in model_names]\n",
    "    \n",
    "    X_trains = [\n",
    "        normalize(sc.sparse.csr.csr_matrix(dataset[\"X_train\"]), norm='l2', axis=1)\n",
    "        for dataset in datasets\n",
    "    ]\n",
    "    X_tests = [\n",
    "        normalize(sc.sparse.csr.csr_matrix(dataset[\"X_test\"]), norm='l2', axis=1)\n",
    "        for dataset in datasets\n",
    "    ]\n",
    "       \n",
    "    concat_dataset = {\n",
    "        \"X_train\": sc.sparse.hstack(X_trains),\n",
    "        \"Y_train\": datasets[0][\"Y_train\"],\n",
    "        \"X_test\": sc.sparse.hstack(X_tests),\n",
    "        \"Y_test\": datasets[0][\"Y_test\"],\n",
    "        \"shas_test\": datasets[0][\"shas_test\"],\n",
    "    }\n",
    "    \n",
    "    print \"Datasets %s: %s = %s\" % (\n",
    "        model_names,\n",
    "        \" + \".join([str(np.shape(dataset[\"X_train\"])[1]) for dataset in datasets]),\n",
    "        np.shape(concat_dataset[\"X_train\"])[1])\n",
    "\n",
    "    print \"Labels equal: %s %s\" % (\n",
    "        [np.array_equal(datasets[0][\"Y_train\"], dataset[\"Y_train\"]) for dataset in datasets[1:]],\n",
    "        [np.array_equal(datasets[0][\"Y_test\"], dataset[\"Y_test\"]) for dataset in datasets[1:]])\n",
    "    \n",
    "    return concat_dataset\n",
    "\n",
    "def test_model(dataset, model_type, train_size, model, model_name, output_errors):\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "    test_pred = model.predict(dataset[\"X_test\"])\n",
    "    test_y = dataset[\"Y_test\"]\n",
    "    shas = dataset[\"shas_test\"]\n",
    "        \n",
    "    accuracy = (float(sum(test_y == test_pred))) / len(test_pred)\n",
    "    precision = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(max(1, sum(test_pred == 1)))\n",
    "    recall = (float(sum((test_y == test_pred) & (test_pred == 1)))) / float(sum(test_y == 1))\n",
    "    f1 = 2 * (precision * recall) / max(1, precision + recall)\n",
    "\n",
    "    print \"%10s %15s. Train set size %5d. %0.1f%% / %0.1f%% / %0.1f%% (%0.3f)\" % (\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size,\n",
    "        accuracy * 100,\n",
    "        precision * 100,\n",
    "        recall * 100,\n",
    "        f1)\n",
    "        \n",
    "    output_table.append([\n",
    "        model_type,\n",
    "        model_name,\n",
    "        train_size, \n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        f1,\n",
    "    ])\n",
    "    \n",
    "    if output_errors:\n",
    "        # Save 10 errors\n",
    "        error_shas = np.array(shas)[test_y != test_pred][0:50]\n",
    "        error_correct = np.array(test_y)[test_y != test_pred][0:50]\n",
    "\n",
    "        with open(\"%s/results/model_errors_%s_%s_%d.txt\" % (REPO_ROOT, model_type, model_name, train_size), \"w\") as fout:\n",
    "            for sha, correct in zip(error_shas, error_correct):\n",
    "                fout.write(\"#### %s FLAG: %s ####\\n\\n\" % (sha, \"Yes\" if correct > 0 else \"No\"))\n",
    "                with open(\"%s/scripts/%s.js\" % (REPO_ROOT, sha), \"r\") as fin:\n",
    "                    fout.write(fin.read())\n",
    "                fout.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var, input_size):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, input_size),\n",
    "                                     input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "        l_in_drop, num_units=20,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "        l_hid1_drop, num_units=10,\n",
    "        nonlinearity=lasagne.nonlinearities.tanh,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hid2_drop, num_units=2,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    \n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert np.shape(inputs)[0] == len(targets)\n",
    "    indices = np.arange(np.shape(inputs)[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, np.shape(inputs)[0] - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        if isinstance(inputs[excerpt], np.ndarray):\n",
    "            i = inputs[excerpt]\n",
    "        else:\n",
    "            i = inputs[excerpt].toarray()\n",
    "        yield i, targets[excerpt]\n",
    "    \n",
    "def test_mlp(dataset, model_type, train_size):\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.lvector('targets')\n",
    "    # Create neural network model\n",
    "    network = build_mlp(input_var, np.shape(dataset[\"X_train\"])[1])\n",
    "    \n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.adam(loss, params)\n",
    "    \n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    \n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "    \n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates, allow_input_downcast=True)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc], allow_input_downcast=True)\n",
    "    \n",
    "    if isinstance(dataset[\"X_train\"], np.ndarray):\n",
    "        X_train_flat = dataset[\"X_train\"]\n",
    "    else:\n",
    "        X_train_flat = dataset[\"X_train\"].tocsc()\n",
    "\n",
    "    if isinstance(dataset[\"X_test\"], np.ndarray):\n",
    "        X_test_flat = dataset[\"X_test\"]\n",
    "    else:\n",
    "        X_test_flat = dataset[\"X_test\"].tocsc()\n",
    "\n",
    "    best_accuracy = 0\n",
    "    bad_count = 0\n",
    "    batch_size = min(200, train_size/10)\n",
    "    for epoch in xrange(999):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train_flat, dataset[\"Y_train\"], batch_size, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_test_flat, dataset[\"Y_test\"], batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "        \n",
    "        current_accuracy = val_acc / val_batches\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} took {:.3f}s - accuracy {:.2f} %\".format(\n",
    "            epoch + 1, time.time() - start_time, current_accuracy * 100))\n",
    "        \n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            bad_count = 0\n",
    "        else:\n",
    "            bad_count += 1\n",
    "            if bad_count > 4:\n",
    "                break\n",
    "        \n",
    "    print \"%10s %15s. Train set size %5d. %0.1f%% / %0.1f%% / %0.1f%% (%0.3f)\" % (\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size,\n",
    "            current_accuracy * 100,\n",
    "            0,\n",
    "            0,\n",
    "            0)\n",
    "    output_table.append([\n",
    "            model_type,\n",
    "            \"MLP\",\n",
    "            train_size, \n",
    "            current_accuracy,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sizes: [300, 600, 1200, 2400, 4800, 9600, 19200]\n",
      "Test size: 3588\n"
     ]
    }
   ],
   "source": [
    "with open(\"%s/model-data/metadata.pickle\" % (REPO_ROOT,), \"r\") as f:\n",
    "    size_data = pickle.load(f)\n",
    "    \n",
    "TRAIN_SIZES = size_data[\"train_sizes\"]\n",
    "TEST_SIZE = size_data[\"test_size\"]\n",
    "\n",
    "print \"Training sizes: %s\" % TRAIN_SIZES\n",
    "print \"Test size: %d\" % TEST_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RegEx             KNN. Train set size   300. 78.0% / 87.3% / 65.7% (0.749)\n",
      "     RegEx       Bernoulli. Train set size   300. 66.0% / 60.9% / 89.2% (0.724)\n",
      "     RegEx             SGD. Train set size   300. 79.7% / 83.5% / 73.9% (0.784)\n",
      "     RegEx    RandomForest. Train set size   300. 81.1% / 84.8% / 75.8% (0.800)\n",
      "     RegEx       LinearSVC. Train set size   300. 79.4% / 84.6% / 71.9% (0.777)\n",
      "   BiRegEx             KNN. Train set size   300. 78.1% / 85.7% / 67.4% (0.755)\n",
      "   BiRegEx       Bernoulli. Train set size   300. 64.0% / 59.1% / 91.3% (0.717)\n",
      "   BiRegEx             SGD. Train set size   300. 79.9% / 83.5% / 74.4% (0.787)\n",
      "   BiRegEx    RandomForest. Train set size   300. 80.7% / 84.4% / 75.4% (0.796)\n",
      "   BiRegEx       LinearSVC. Train set size   300. 79.5% / 84.1% / 72.6% (0.780)\n",
      "  TriRegEx             KNN. Train set size   300. 78.8% / 87.1% / 67.5% (0.761)\n",
      "  TriRegEx       Bernoulli. Train set size   300. 63.2% / 58.2% / 93.4% (0.717)\n",
      "  TriRegEx             SGD. Train set size   300. 80.9% / 84.0% / 76.4% (0.800)\n",
      "  TriRegEx    RandomForest. Train set size   300. 79.9% / 82.0% / 76.6% (0.792)\n",
      "  TriRegEx       LinearSVC. Train set size   300. 80.1% / 84.5% / 73.7% (0.787)\n",
      "       AST             KNN. Train set size   300. 77.1% / 84.1% / 67.0% (0.746)\n",
      "       AST       Bernoulli. Train set size   300. 64.9% / 59.9% / 89.9% (0.719)\n",
      "       AST             SGD. Train set size   300. 78.8% / 80.9% / 75.6% (0.781)\n",
      "       AST    RandomForest. Train set size   300. 79.1% / 81.3% / 75.5% (0.783)\n",
      "       AST       LinearSVC. Train set size   300. 79.0% / 83.1% / 72.7% (0.776)\n",
      "     BiAST             KNN. Train set size   300. 78.8% / 85.1% / 69.9% (0.768)\n",
      "     BiAST       Bernoulli. Train set size   300. 61.6% / 57.3% / 91.2% (0.704)\n",
      "     BiAST             SGD. Train set size   300. 80.2% / 83.9% / 74.6% (0.790)\n",
      "     BiAST    RandomForest. Train set size   300. 77.9% / 79.0% / 76.0% (0.775)\n",
      "     BiAST       LinearSVC. Train set size   300. 79.9% / 85.1% / 72.5% (0.783)\n",
      "    TriAST             KNN. Train set size   300. 79.2% / 86.7% / 69.0% (0.768)\n",
      "    TriAST       Bernoulli. Train set size   300. 60.6% / 56.3% / 93.9% (0.704)\n",
      "    TriAST             SGD. Train set size   300. 80.2% / 84.1% / 74.4% (0.789)\n",
      "    TriAST    RandomForest. Train set size   300. 76.9% / 75.8% / 79.1% (0.774)\n",
      "    TriAST       LinearSVC. Train set size   300. 80.1% / 85.5% / 72.5% (0.785)\n",
      "Random2Vec             KNN. Train set size   300. 74.1% / 86.1% / 57.4% (0.689)\n",
      "Random2Vec       Bernoulli. Train set size   300. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size   300. 69.9% / 70.5% / 68.4% (0.694)\n",
      "Random2Vec    RandomForest. Train set size   300. 71.7% / 73.2% / 68.5% (0.707)\n",
      "Random2Vec       LinearSVC. Train set size   300. 67.3% / 71.0% / 58.6% (0.642)\n",
      "  Word2Vec             KNN. Train set size   300. 75.6% / 84.7% / 62.4% (0.719)\n",
      "  Word2Vec       Bernoulli. Train set size   300. 70.0% / 77.1% / 57.0% (0.656)\n",
      "  Word2Vec             SGD. Train set size   300. 72.4% / 76.5% / 64.8% (0.701)\n",
      "  Word2Vec    RandomForest. Train set size   300. 79.1% / 82.9% / 73.2% (0.778)\n",
      "  Word2Vec       LinearSVC. Train set size   300. 74.9% / 73.5% / 77.8% (0.756)\n",
      "   AST2Vec             KNN. Train set size   300. 76.2% / 84.0% / 64.8% (0.732)\n",
      "   AST2Vec       Bernoulli. Train set size   300. 70.3% / 81.6% / 52.5% (0.639)\n",
      "   AST2Vec             SGD. Train set size   300. 75.1% / 74.3% / 76.8% (0.755)\n",
      "   AST2Vec    RandomForest. Train set size   300. 79.9% / 86.1% / 71.3% (0.780)\n",
      "   AST2Vec       LinearSVC. Train set size   300. 67.5% / 68.6% / 64.6% (0.665)\n",
      "     RegEx             KNN. Train set size   600. 79.8% / 87.4% / 69.6% (0.775)\n",
      "     RegEx       Bernoulli. Train set size   600. 66.0% / 60.9% / 89.3% (0.724)\n",
      "     RegEx             SGD. Train set size   600. 81.6% / 86.2% / 75.3% (0.803)\n",
      "     RegEx    RandomForest. Train set size   600. 81.8% / 85.0% / 77.2% (0.809)\n",
      "     RegEx       LinearSVC. Train set size   600. 81.5% / 86.7% / 74.4% (0.801)\n",
      "   BiRegEx             KNN. Train set size   600. 80.6% / 87.2% / 71.7% (0.787)\n",
      "   BiRegEx       Bernoulli. Train set size   600. 64.2% / 59.3% / 91.1% (0.718)\n",
      "   BiRegEx             SGD. Train set size   600. 81.1% / 85.6% / 74.9% (0.799)\n",
      "   BiRegEx    RandomForest. Train set size   600. 81.2% / 83.9% / 77.3% (0.804)\n",
      "   BiRegEx       LinearSVC. Train set size   600. 81.3% / 86.7% / 74.0% (0.798)\n",
      "  TriRegEx             KNN. Train set size   600. 81.2% / 87.2% / 73.2% (0.796)\n",
      "  TriRegEx       Bernoulli. Train set size   600. 64.0% / 58.7% / 93.9% (0.723)\n",
      "  TriRegEx             SGD. Train set size   600. 82.6% / 86.7% / 77.1% (0.816)\n",
      "  TriRegEx    RandomForest. Train set size   600. 79.4% / 78.9% / 80.3% (0.796)\n",
      "  TriRegEx       LinearSVC. Train set size   600. 82.3% / 87.4% / 75.5% (0.810)\n",
      "       AST             KNN. Train set size   600. 80.5% / 88.7% / 69.9% (0.782)\n",
      "       AST       Bernoulli. Train set size   600. 65.0% / 59.7% / 91.8% (0.724)\n",
      "       AST             SGD. Train set size   600. 81.9% / 85.5% / 76.9% (0.810)\n",
      "       AST    RandomForest. Train set size   600. 80.4% / 82.2% / 77.7% (0.799)\n",
      "       AST       LinearSVC. Train set size   600. 81.2% / 85.8% / 74.7% (0.799)\n",
      "     BiAST             KNN. Train set size   600. 81.5% / 91.0% / 70.0% (0.791)\n",
      "     BiAST       Bernoulli. Train set size   600. 62.3% / 57.6% / 93.1% (0.712)\n",
      "     BiAST             SGD. Train set size   600. 82.4% / 86.5% / 76.7% (0.813)\n",
      "     BiAST    RandomForest. Train set size   600. 78.6% / 79.3% / 77.4% (0.783)\n",
      "     BiAST       LinearSVC. Train set size   600. 82.2% / 87.1% / 75.7% (0.810)\n",
      "    TriAST             KNN. Train set size   600. 82.0% / 93.1% / 69.1% (0.793)\n",
      "    TriAST       Bernoulli. Train set size   600. 60.7% / 56.4% / 94.7% (0.707)\n",
      "    TriAST             SGD. Train set size   600. 82.6% / 87.1% / 76.4% (0.814)\n",
      "    TriAST    RandomForest. Train set size   600. 77.5% / 76.3% / 79.7% (0.780)\n",
      "    TriAST       LinearSVC. Train set size   600. 82.8% / 87.8% / 76.1% (0.815)\n",
      "Random2Vec             KNN. Train set size   600. 76.2% / 87.4% / 61.2% (0.720)\n",
      "Random2Vec       Bernoulli. Train set size   600. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size   600. 63.6% / 66.9% / 53.9% (0.597)\n",
      "Random2Vec    RandomForest. Train set size   600. 76.0% / 79.3% / 70.5% (0.746)\n",
      "Random2Vec       LinearSVC. Train set size   600. 70.9% / 67.6% / 80.2% (0.734)\n",
      "  Word2Vec             KNN. Train set size   600. 77.4% / 84.6% / 67.8% (0.753)\n",
      "  Word2Vec       Bernoulli. Train set size   600. 66.8% / 74.5% / 52.9% (0.619)\n",
      "  Word2Vec             SGD. Train set size   600. 78.8% / 79.5% / 78.7% (0.791)\n",
      "  Word2Vec    RandomForest. Train set size   600. 81.9% / 83.7% / 79.9% (0.817)\n",
      "  Word2Vec       LinearSVC. Train set size   600. 78.1% / 77.1% / 81.0% (0.790)\n",
      "   AST2Vec             KNN. Train set size   600. 78.6% / 82.9% / 72.9% (0.776)\n",
      "   AST2Vec       Bernoulli. Train set size   600. 69.5% / 82.1% / 51.3% (0.631)\n",
      "   AST2Vec             SGD. Train set size   600. 75.4% / 70.3% / 89.4% (0.787)\n",
      "   AST2Vec    RandomForest. Train set size   600. 80.9% / 80.3% / 82.8% (0.815)\n",
      "   AST2Vec       LinearSVC. Train set size   600. 75.1% / 72.3% / 82.7% (0.771)\n",
      "     RegEx             KNN. Train set size  1200. 82.0% / 86.8% / 75.5% (0.808)\n",
      "     RegEx       Bernoulli. Train set size  1200. 65.9% / 60.8% / 89.9% (0.725)\n",
      "     RegEx             SGD. Train set size  1200. 84.0% / 87.9% / 78.8% (0.831)\n",
      "     RegEx    RandomForest. Train set size  1200. 82.2% / 84.1% / 79.3% (0.816)\n",
      "     RegEx       LinearSVC. Train set size  1200. 84.5% / 88.8% / 79.0% (0.836)\n",
      "   BiRegEx             KNN. Train set size  1200. 79.7% / 79.1% / 80.8% (0.799)\n",
      "   BiRegEx       Bernoulli. Train set size  1200. 65.2% / 59.8% / 92.6% (0.727)\n",
      "   BiRegEx             SGD. Train set size  1200. 84.6% / 87.9% / 80.1% (0.838)\n",
      "   BiRegEx    RandomForest. Train set size  1200. 78.5% / 76.9% / 81.5% (0.791)\n",
      "   BiRegEx       LinearSVC. Train set size  1200. 85.2% / 89.0% / 80.4% (0.845)\n",
      "  TriRegEx             KNN. Train set size  1200. 77.6% / 74.3% / 84.3% (0.790)\n",
      "  TriRegEx       Bernoulli. Train set size  1200. 64.5% / 59.1% / 94.0% (0.726)\n",
      "  TriRegEx             SGD. Train set size  1200. 85.5% / 89.1% / 80.9% (0.848)\n",
      "  TriRegEx    RandomForest. Train set size  1200. 77.8% / 75.7% / 81.8% (0.786)\n",
      "  TriRegEx       LinearSVC. Train set size  1200. 85.8% / 90.2% / 80.4% (0.850)\n",
      "       AST             KNN. Train set size  1200. 83.1% / 91.9% / 72.6% (0.811)\n",
      "       AST       Bernoulli. Train set size  1200. 65.3% / 59.8% / 93.6% (0.730)\n",
      "       AST             SGD. Train set size  1200. 83.9% / 88.5% / 78.0% (0.829)\n",
      "       AST    RandomForest. Train set size  1200. 79.0% / 78.3% / 80.3% (0.793)\n",
      "       AST       LinearSVC. Train set size  1200. 84.4% / 89.5% / 77.8% (0.833)\n",
      "     BiAST             KNN. Train set size  1200. 83.8% / 93.2% / 72.9% (0.818)\n",
      "     BiAST       Bernoulli. Train set size  1200. 62.9% / 58.0% / 94.0% (0.717)\n",
      "     BiAST             SGD. Train set size  1200. 84.3% / 89.2% / 78.0% (0.833)\n",
      "     BiAST    RandomForest. Train set size  1200. 76.2% / 73.2% / 82.6% (0.776)\n",
      "     BiAST       LinearSVC. Train set size  1200. 84.7% / 90.1% / 77.9% (0.836)\n",
      "    TriAST             KNN. Train set size  1200. 84.3% / 94.4% / 73.0% (0.823)\n",
      "    TriAST       Bernoulli. Train set size  1200. 61.6% / 57.0% / 94.4% (0.711)\n",
      "    TriAST             SGD. Train set size  1200. 84.7% / 89.6% / 78.5% (0.837)\n",
      "    TriAST    RandomForest. Train set size  1200. 75.0% / 70.8% / 85.1% (0.773)\n",
      "    TriAST       LinearSVC. Train set size  1200. 85.0% / 90.2% / 78.6% (0.840)\n",
      "Random2Vec             KNN. Train set size  1200. 78.2% / 85.7% / 67.7% (0.756)\n",
      "Random2Vec       Bernoulli. Train set size  1200. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  1200. 55.4% / 70.7% / 18.6% (0.262)\n",
      "Random2Vec    RandomForest. Train set size  1200. 80.7% / 84.5% / 75.0% (0.795)\n",
      "Random2Vec       LinearSVC. Train set size  1200. 71.1% / 83.8% / 52.3% (0.644)\n",
      "  Word2Vec             KNN. Train set size  1200. 80.4% / 85.9% / 73.5% (0.792)\n",
      "  Word2Vec       Bernoulli. Train set size  1200. 66.5% / 74.4% / 51.8% (0.610)\n",
      "  Word2Vec             SGD. Train set size  1200. 80.2% / 78.1% / 84.6% (0.812)\n",
      "  Word2Vec    RandomForest. Train set size  1200. 82.9% / 85.9% / 79.2% (0.824)\n",
      "  Word2Vec       LinearSVC. Train set size  1200. 79.4% / 76.9% / 84.8% (0.807)\n",
      "   AST2Vec             KNN. Train set size  1200. 81.4% / 87.0% / 74.5% (0.802)\n",
      "   AST2Vec       Bernoulli. Train set size  1200. 67.0% / 80.8% / 45.9% (0.585)\n",
      "   AST2Vec             SGD. Train set size  1200. 73.9% / 79.5% / 65.5% (0.718)\n",
      "   AST2Vec    RandomForest. Train set size  1200. 82.7% / 83.4% / 82.2% (0.828)\n",
      "   AST2Vec       LinearSVC. Train set size  1200. 75.0% / 69.7% / 89.6% (0.784)\n",
      "     RegEx             KNN. Train set size  2400. 83.8% / 87.8% / 78.4% (0.828)\n",
      "     RegEx       Bernoulli. Train set size  2400. 66.7% / 61.3% / 90.4% (0.731)\n",
      "     RegEx             SGD. Train set size  2400. 85.1% / 88.2% / 81.1% (0.845)\n",
      "     RegEx    RandomForest. Train set size  2400. 83.4% / 84.3% / 82.1% (0.832)\n",
      "     RegEx       LinearSVC. Train set size  2400. 86.3% / 89.9% / 81.9% (0.857)\n",
      "   BiRegEx             KNN. Train set size  2400. 81.8% / 81.3% / 82.6% (0.819)\n",
      "   BiRegEx       Bernoulli. Train set size  2400. 65.3% / 59.9% / 92.5% (0.727)\n",
      "   BiRegEx             SGD. Train set size  2400. 85.5% / 89.2% / 80.7% (0.847)\n",
      "   BiRegEx    RandomForest. Train set size  2400. 77.8% / 74.2% / 85.4% (0.794)\n",
      "   BiRegEx       LinearSVC. Train set size  2400. 87.3% / 90.9% / 82.9% (0.867)\n",
      "  TriRegEx             KNN. Train set size  2400. 79.7% / 76.6% / 85.7% (0.809)\n",
      "  TriRegEx       Bernoulli. Train set size  2400. 64.8% / 59.3% / 94.3% (0.728)\n",
      "  TriRegEx             SGD. Train set size  2400. 87.0% / 91.2% / 81.9% (0.863)\n",
      "  TriRegEx    RandomForest. Train set size  2400. 77.6% / 73.9% / 85.6% (0.793)\n",
      "  TriRegEx       LinearSVC. Train set size  2400. 87.8% / 91.5% / 83.5% (0.873)\n",
      "       AST             KNN. Train set size  2400. 85.0% / 92.9% / 75.9% (0.835)\n",
      "       AST       Bernoulli. Train set size  2400. 65.6% / 59.9% / 95.0% (0.734)\n",
      "       AST             SGD. Train set size  2400. 84.7% / 89.0% / 79.3% (0.838)\n",
      "       AST    RandomForest. Train set size  2400. 79.1% / 77.4% / 82.2% (0.797)\n",
      "       AST       LinearSVC. Train set size  2400. 86.0% / 90.0% / 81.0% (0.853)\n",
      "     BiAST             KNN. Train set size  2400. 86.1% / 95.4% / 75.9% (0.845)\n",
      "     BiAST       Bernoulli. Train set size  2400. 63.4% / 58.2% / 95.1% (0.722)\n",
      "     BiAST             SGD. Train set size  2400. 85.9% / 89.9% / 80.9% (0.852)\n",
      "     BiAST    RandomForest. Train set size  2400. 74.9% / 70.4% / 86.0% (0.774)\n",
      "     BiAST       LinearSVC. Train set size  2400. 86.9% / 91.1% / 81.8% (0.862)\n",
      "    TriAST             KNN. Train set size  2400. 86.5% / 95.3% / 76.8% (0.850)\n",
      "    TriAST       Bernoulli. Train set size  2400. 62.2% / 57.4% / 94.1% (0.713)\n",
      "    TriAST             SGD. Train set size  2400. 86.5% / 90.8% / 81.2% (0.857)\n",
      "    TriAST    RandomForest. Train set size  2400. 76.5% / 73.0% / 84.1% (0.782)\n",
      "    TriAST       LinearSVC. Train set size  2400. 87.8% / 92.0% / 82.7% (0.871)\n",
      "Random2Vec             KNN. Train set size  2400. 80.2% / 87.2% / 70.8% (0.782)\n",
      "Random2Vec       Bernoulli. Train set size  2400. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  2400. 73.9% / 75.2% / 71.5% (0.733)\n",
      "Random2Vec    RandomForest. Train set size  2400. 81.9% / 85.4% / 77.0% (0.810)\n",
      "Random2Vec       LinearSVC. Train set size  2400. 71.4% / 77.3% / 60.6% (0.680)\n",
      "  Word2Vec             KNN. Train set size  2400. 83.5% / 90.0% / 75.1% (0.819)\n",
      "  Word2Vec       Bernoulli. Train set size  2400. 69.4% / 75.2% / 57.1% (0.649)\n",
      "  Word2Vec             SGD. Train set size  2400. 80.4% / 76.3% / 87.6% (0.816)\n",
      "  Word2Vec    RandomForest. Train set size  2400. 85.8% / 91.0% / 79.3% (0.847)\n",
      "  Word2Vec       LinearSVC. Train set size  2400. 79.5% / 78.0% / 81.6% (0.798)\n",
      "   AST2Vec             KNN. Train set size  2400. 82.5% / 90.0% / 72.8% (0.805)\n",
      "   AST2Vec       Bernoulli. Train set size  2400. 70.4% / 82.3% / 51.3% (0.632)\n",
      "   AST2Vec             SGD. Train set size  2400. 76.1% / 74.0% / 79.9% (0.768)\n",
      "   AST2Vec    RandomForest. Train set size  2400. 86.0% / 92.1% / 78.4% (0.847)\n",
      "   AST2Vec       LinearSVC. Train set size  2400. 75.7% / 79.3% / 69.0% (0.738)\n",
      "     RegEx             KNN. Train set size  4800. 85.9% / 90.0% / 80.8% (0.852)\n",
      "     RegEx       Bernoulli. Train set size  4800. 66.4% / 61.1% / 90.5% (0.729)\n",
      "     RegEx             SGD. Train set size  4800. 86.0% / 90.0% / 81.0% (0.853)\n",
      "     RegEx    RandomForest. Train set size  4800. 80.4% / 76.6% / 87.6% (0.817)\n",
      "     RegEx       LinearSVC. Train set size  4800. 88.4% / 91.5% / 84.7% (0.880)\n",
      "   BiRegEx             KNN. Train set size  4800. 83.9% / 83.1% / 85.1% (0.841)\n",
      "   BiRegEx       Bernoulli. Train set size  4800. 65.9% / 60.3% / 93.4% (0.733)\n",
      "   BiRegEx             SGD. Train set size  4800. 87.6% / 91.9% / 82.5% (0.869)\n",
      "   BiRegEx    RandomForest. Train set size  4800. 77.0% / 72.2% / 87.6% (0.792)\n",
      "   BiRegEx       LinearSVC. Train set size  4800. 89.7% / 92.6% / 86.3% (0.893)\n",
      "  TriRegEx             KNN. Train set size  4800. 81.7% / 78.6% / 87.1% (0.826)\n",
      "  TriRegEx       Bernoulli. Train set size  4800. 65.5% / 59.8% / 94.3% (0.732)\n",
      "  TriRegEx             SGD. Train set size  4800. 88.3% / 92.7% / 83.1% (0.876)\n",
      "  TriRegEx    RandomForest. Train set size  4800. 79.2% / 75.4% / 86.6% (0.806)\n",
      "  TriRegEx       LinearSVC. Train set size  4800. 90.1% / 92.9% / 87.0% (0.898)\n",
      "       AST             KNN. Train set size  4800. 87.7% / 94.0% / 80.5% (0.867)\n",
      "       AST       Bernoulli. Train set size  4800. 65.4% / 59.6% / 95.5% (0.734)\n",
      "       AST             SGD. Train set size  4800. 86.6% / 91.7% / 80.6% (0.858)\n",
      "       AST    RandomForest. Train set size  4800. 76.3% / 71.6% / 87.1% (0.786)\n",
      "       AST       LinearSVC. Train set size  4800. 88.4% / 91.4% / 84.8% (0.880)\n",
      "     BiAST             KNN. Train set size  4800. 87.9% / 94.8% / 80.2% (0.869)\n",
      "     BiAST       Bernoulli. Train set size  4800. 63.9% / 58.5% / 95.4% (0.725)\n",
      "     BiAST             SGD. Train set size  4800. 88.0% / 92.2% / 82.9% (0.873)\n",
      "     BiAST    RandomForest. Train set size  4800. 72.9% / 67.6% / 87.8% (0.764)\n",
      "     BiAST       LinearSVC. Train set size  4800. 89.0% / 91.8% / 85.6% (0.886)\n",
      "    TriAST             KNN. Train set size  4800. 88.2% / 95.1% / 80.5% (0.872)\n",
      "    TriAST       Bernoulli. Train set size  4800. 62.9% / 57.9% / 94.2% (0.717)\n",
      "    TriAST             SGD. Train set size  4800. 87.9% / 91.9% / 83.2% (0.873)\n",
      "    TriAST    RandomForest. Train set size  4800. 77.2% / 73.7% / 84.7% (0.788)\n",
      "    TriAST       LinearSVC. Train set size  4800. 89.3% / 92.0% / 86.2% (0.890)\n",
      "Random2Vec             KNN. Train set size  4800. 83.3% / 90.2% / 74.7% (0.817)\n",
      "Random2Vec       Bernoulli. Train set size  4800. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  4800. 72.6% / 72.9% / 72.0% (0.724)\n",
      "Random2Vec    RandomForest. Train set size  4800. 84.1% / 88.8% / 78.1% (0.831)\n",
      "Random2Vec       LinearSVC. Train set size  4800. 78.2% / 75.4% / 83.8% (0.794)\n",
      "  Word2Vec             KNN. Train set size  4800. 84.1% / 89.9% / 76.7% (0.828)\n",
      "  Word2Vec       Bernoulli. Train set size  4800. 67.4% / 73.0% / 55.1% (0.628)\n",
      "  Word2Vec             SGD. Train set size  4800. 80.6% / 77.8% / 85.5% (0.815)\n",
      "  Word2Vec    RandomForest. Train set size  4800. 87.7% / 93.0% / 81.4% (0.868)\n",
      "  Word2Vec       LinearSVC. Train set size  4800. 75.4% / 79.3% / 68.9% (0.737)\n",
      "   AST2Vec             KNN. Train set size  4800. 84.6% / 89.8% / 78.0% (0.835)\n",
      "   AST2Vec       Bernoulli. Train set size  4800. 68.1% / 79.8% / 48.6% (0.604)\n",
      "   AST2Vec             SGD. Train set size  4800. 60.8% / 56.3% / 97.3% (0.713)\n",
      "   AST2Vec    RandomForest. Train set size  4800. 88.0% / 93.0% / 82.3% (0.873)\n",
      "   AST2Vec       LinearSVC. Train set size  4800. 79.5% / 78.0% / 82.2% (0.801)\n",
      "     RegEx             KNN. Train set size  9600. 87.2% / 90.0% / 83.8% (0.868)\n",
      "     RegEx       Bernoulli. Train set size  9600. 67.1% / 61.6% / 90.6% (0.734)\n",
      "     RegEx             SGD. Train set size  9600. 85.6% / 88.8% / 81.4% (0.849)\n",
      "     RegEx    RandomForest. Train set size  9600. 80.0% / 75.7% / 88.5% (0.816)\n",
      "     RegEx       LinearSVC. Train set size  9600. 89.9% / 92.8% / 86.5% (0.895)\n",
      "   BiRegEx             KNN. Train set size  9600. 85.4% / 84.3% / 87.0% (0.856)\n",
      "   BiRegEx       Bernoulli. Train set size  9600. 66.2% / 60.5% / 93.4% (0.735)\n",
      "   BiRegEx             SGD. Train set size  9600. 87.5% / 91.7% / 82.4% (0.868)\n",
      "   BiRegEx    RandomForest. Train set size  9600. 75.6% / 69.8% / 90.4% (0.788)\n",
      "   BiRegEx       LinearSVC. Train set size  9600. 91.0% / 93.4% / 88.3% (0.908)\n",
      "  TriRegEx             KNN. Train set size  9600. 83.7% / 80.6% / 88.9% (0.845)\n",
      "  TriRegEx       Bernoulli. Train set size  9600. 66.0% / 60.2% / 94.1% (0.734)\n",
      "  TriRegEx             SGD. Train set size  9600. 89.0% / 92.8% / 84.6% (0.885)\n",
      "  TriRegEx    RandomForest. Train set size  9600. 79.1% / 74.8% / 87.6% (0.807)\n",
      "  TriRegEx       LinearSVC. Train set size  9600. 91.2% / 93.4% / 88.8% (0.910)\n",
      "       AST             KNN. Train set size  9600. 89.4% / 94.9% / 83.2% (0.887)\n",
      "       AST       Bernoulli. Train set size  9600. 66.0% / 60.0% / 95.8% (0.738)\n",
      "       AST             SGD. Train set size  9600. 87.4% / 92.2% / 81.8% (0.867)\n",
      "       AST    RandomForest. Train set size  9600. 74.7% / 68.7% / 90.7% (0.782)\n",
      "       AST       LinearSVC. Train set size  9600. 90.0% / 92.0% / 87.6% (0.898)\n",
      "     BiAST             KNN. Train set size  9600. 88.9% / 95.2% / 81.9% (0.881)\n",
      "     BiAST       Bernoulli. Train set size  9600. 64.1% / 58.7% / 94.8% (0.725)\n",
      "     BiAST             SGD. Train set size  9600. 88.1% / 92.8% / 82.6% (0.874)\n",
      "     BiAST    RandomForest. Train set size  9600. 73.0% / 67.7% / 88.0% (0.765)\n",
      "     BiAST       LinearSVC. Train set size  9600. 90.5% / 92.7% / 88.0% (0.903)\n",
      "    TriAST             KNN. Train set size  9600. 89.4% / 95.6% / 82.5% (0.886)\n",
      "    TriAST       Bernoulli. Train set size  9600. 63.1% / 58.1% / 94.0% (0.718)\n",
      "    TriAST             SGD. Train set size  9600. 88.2% / 92.3% / 83.3% (0.876)\n",
      "    TriAST    RandomForest. Train set size  9600. 76.1% / 71.5% / 86.8% (0.784)\n",
      "    TriAST       LinearSVC. Train set size  9600. 90.6% / 92.6% / 88.2% (0.904)\n",
      "Random2Vec             KNN. Train set size  9600. 84.6% / 90.2% / 77.8% (0.835)\n",
      "Random2Vec       Bernoulli. Train set size  9600. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size  9600. 80.9% / 78.4% / 85.4% (0.817)\n",
      "Random2Vec    RandomForest. Train set size  9600. 86.2% / 91.1% / 80.3% (0.853)\n",
      "Random2Vec       LinearSVC. Train set size  9600. 58.7% / 93.3% / 18.7% (0.311)\n",
      "  Word2Vec             KNN. Train set size  9600. 87.2% / 92.1% / 81.7% (0.866)\n",
      "  Word2Vec       Bernoulli. Train set size  9600. 67.6% / 74.2% / 54.9% (0.631)\n",
      "  Word2Vec             SGD. Train set size  9600. 77.3% / 73.8% / 85.3% (0.791)\n",
      "  Word2Vec    RandomForest. Train set size  9600. 88.6% / 93.4% / 83.2% (0.880)\n",
      "  Word2Vec       LinearSVC. Train set size  9600. 77.7% / 81.7% / 71.9% (0.765)\n",
      "   AST2Vec             KNN. Train set size  9600. 86.6% / 90.4% / 82.2% (0.861)\n",
      "   AST2Vec       Bernoulli. Train set size  9600. 69.5% / 83.3% / 49.5% (0.621)\n",
      "   AST2Vec             SGD. Train set size  9600. 78.5% / 72.9% / 91.4% (0.811)\n",
      "   AST2Vec    RandomForest. Train set size  9600. 89.6% / 95.1% / 83.7% (0.890)\n",
      "   AST2Vec       LinearSVC. Train set size  9600. 83.0% / 84.6% / 81.1% (0.828)\n",
      "     RegEx             KNN. Train set size 19200. 89.1% / 91.5% / 86.3% (0.888)\n",
      "     RegEx       Bernoulli. Train set size 19200. 67.4% / 61.7% / 91.6% (0.738)\n",
      "     RegEx             SGD. Train set size 19200. 85.6% / 89.2% / 81.0% (0.849)\n",
      "     RegEx    RandomForest. Train set size 19200. 76.9% / 71.5% / 89.4% (0.795)\n",
      "     RegEx       LinearSVC. Train set size 19200. 90.8% / 93.5% / 87.7% (0.905)\n",
      "   BiRegEx             KNN. Train set size 19200. 86.5% / 85.3% / 88.2% (0.867)\n",
      "   BiRegEx       Bernoulli. Train set size 19200. 67.0% / 61.1% / 93.4% (0.739)\n",
      "   BiRegEx             SGD. Train set size 19200. 88.2% / 92.7% / 82.9% (0.875)\n",
      "   BiRegEx    RandomForest. Train set size 19200. 75.4% / 69.4% / 90.9% (0.787)\n",
      "   BiRegEx       LinearSVC. Train set size 19200. 92.1% / 93.8% / 90.1% (0.919)\n",
      "  TriRegEx             KNN. Train set size 19200. 85.7% / 83.1% / 89.7% (0.863)\n",
      "  TriRegEx       Bernoulli. Train set size 19200. 66.2% / 60.4% / 93.7% (0.735)\n",
      "  TriRegEx             SGD. Train set size 19200. 88.8% / 93.0% / 84.0% (0.883)\n",
      "  TriRegEx    RandomForest. Train set size 19200. 78.8% / 74.4% / 87.8% (0.805)\n",
      "  TriRegEx       LinearSVC. Train set size 19200. 92.4% / 94.0% / 90.6% (0.923)\n",
      "       AST             KNN. Train set size 19200. 90.4% / 95.9% / 84.3% (0.897)\n",
      "       AST       Bernoulli. Train set size 19200. 66.4% / 60.3% / 96.2% (0.741)\n",
      "       AST             SGD. Train set size 19200. 87.6% / 92.7% / 81.6% (0.868)\n",
      "       AST    RandomForest. Train set size 19200. 72.3% / 65.8% / 92.7% (0.770)\n",
      "       AST       LinearSVC. Train set size 19200. 91.3% / 93.3% / 89.1% (0.911)\n",
      "     BiAST             KNN. Train set size 19200. 90.2% / 95.7% / 84.3% (0.896)\n",
      "     BiAST       Bernoulli. Train set size 19200. 64.5% / 59.0% / 94.6% (0.727)\n",
      "     BiAST             SGD. Train set size 19200. 88.3% / 93.2% / 82.7% (0.876)\n",
      "     BiAST    RandomForest. Train set size 19200. 74.7% / 69.9% / 87.1% (0.775)\n",
      "     BiAST       LinearSVC. Train set size 19200. 91.5% / 93.5% / 89.1% (0.913)\n",
      "    TriAST             KNN. Train set size 19200. 87.9% / 96.9% / 78.3% (0.866)\n",
      "    TriAST       Bernoulli. Train set size 19200. 62.5% / 57.9% / 91.5% (0.709)\n",
      "    TriAST             SGD. Train set size 19200. 88.6% / 93.2% / 83.3% (0.880)\n",
      "    TriAST    RandomForest. Train set size 19200. 77.6% / 73.2% / 87.2% (0.796)\n",
      "    TriAST       LinearSVC. Train set size 19200. 91.8% / 94.1% / 89.2% (0.916)\n",
      "Random2Vec             KNN. Train set size 19200. 86.6% / 91.6% / 80.7% (0.858)\n",
      "Random2Vec       Bernoulli. Train set size 19200. 50.0% / 54.5% / 0.3% (0.004)\n",
      "Random2Vec             SGD. Train set size 19200. 79.0% / 81.3% / 75.3% (0.782)\n",
      "Random2Vec    RandomForest. Train set size 19200. 87.5% / 92.8% / 81.2% (0.866)\n",
      "Random2Vec       LinearSVC. Train set size 19200. 75.9% / 83.7% / 64.4% (0.728)\n",
      "  Word2Vec             KNN. Train set size 19200. 88.1% / 92.6% / 82.8% (0.875)\n",
      "  Word2Vec       Bernoulli. Train set size 19200. 70.0% / 77.8% / 55.9% (0.650)\n",
      "  Word2Vec             SGD. Train set size 19200. 77.5% / 72.1% / 89.7% (0.800)\n",
      "  Word2Vec    RandomForest. Train set size 19200. 90.0% / 94.8% / 84.7% (0.895)\n",
      "  Word2Vec       LinearSVC. Train set size 19200. 83.8% / 89.0% / 77.0% (0.826)\n",
      "   AST2Vec             KNN. Train set size 19200. 88.5% / 92.6% / 83.7% (0.879)\n",
      "   AST2Vec       Bernoulli. Train set size 19200. 70.4% / 83.1% / 51.2% (0.634)\n",
      "   AST2Vec             SGD. Train set size 19200. 68.4% / 61.6% / 97.5% (0.755)\n",
      "   AST2Vec    RandomForest. Train set size 19200. 90.7% / 95.9% / 84.9% (0.901)\n",
      "   AST2Vec       LinearSVC. Train set size 19200. 82.3% / 85.7% / 77.5% (0.814)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    for model_type in [\"RegEx\", \"BiRegEx\", \"TriRegEx\", \"AST\", \"BiAST\", \"TriAST\", \"Random2Vec\", \"Word2Vec\", \"AST2Vec\"]:\n",
    "        dataset = load_model(model_type, train_size)\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   KNeighborsClassifier(2), \"KNN\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   BernoulliNB(), \"Bernoulli\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   LinearSVC(), \"LinearSVC\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "                \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/linear_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BiRegEx1K       LinearSVC. Train set size 19200. 91.8% / 94.4% / 88.8% (0.915)\n",
      " BiRegEx4K       LinearSVC. Train set size 19200. 91.8% / 93.9% / 89.4% (0.916)\n",
      "BiRegEx16K       LinearSVC. Train set size 19200. 92.1% / 93.9% / 89.9% (0.919)\n",
      "BiRegEx64K       LinearSVC. Train set size 19200. 92.1% / 94.0% / 90.1% (0.920)\n",
      "BiRegEx256K       LinearSVC. Train set size 19200. 92.1% / 93.9% / 90.0% (0.919)\n",
      " BiRegEx1M       LinearSVC. Train set size 19200. 92.1% / 93.8% / 90.1% (0.919)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "for model_type in [\"BiRegEx1K\", \"BiRegEx4K\", \"BiRegEx16K\", \"BiRegEx64K\", \"BiRegEx256K\", \"BiRegEx1M\"]:\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    test_model(dataset, model_type, train_size,\n",
    "               LinearSVC(), \"LinearSVC\",\n",
    "               train_size == TRAIN_SIZES[-1])\n",
    "                \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/truncated_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "for model_type in [\"RegEx\", \"TriRegEx\", \"AST\", \"TriAST\", \"AST2Vec\"]:\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    test_mlp(dataset, model_type, train_size)\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/mlp_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Url3             KNN. Train set size   300. 80.5% / 82.0% / 78.2% (0.800)\n",
      "      Url3       Bernoulli. Train set size   300. 71.8% / 92.6% / 47.3% (0.626)\n",
      "      Url3             SGD. Train set size   300. 83.8% / 86.1% / 80.6% (0.833)\n",
      "      Url3    RandomForest. Train set size   300. 81.2% / 92.1% / 68.2% (0.784)\n",
      "      Url3       LinearSVC. Train set size   300. 84.0% / 86.9% / 80.1% (0.834)\n",
      "      Url6             KNN. Train set size   300. 81.9% / 86.7% / 75.5% (0.807)\n",
      "      Url6       Bernoulli. Train set size   300. 70.0% / 97.4% / 41.2% (0.579)\n",
      "      Url6             SGD. Train set size   300. 85.8% / 89.4% / 81.3% (0.852)\n",
      "      Url6    RandomForest. Train set size   300. 80.0% / 91.8% / 65.9% (0.767)\n",
      "      Url6       LinearSVC. Train set size   300. 85.7% / 89.4% / 80.9% (0.850)\n",
      "     Url12             KNN. Train set size   300. 82.2% / 88.8% / 73.6% (0.805)\n",
      "     Url12       Bernoulli. Train set size   300. 68.7% / 99.6% / 37.6% (0.546)\n",
      "     Url12             SGD. Train set size   300. 86.0% / 90.5% / 80.3% (0.851)\n",
      "     Url12    RandomForest. Train set size   300. 79.7% / 97.5% / 61.0% (0.751)\n",
      "     Url12       LinearSVC. Train set size   300. 86.1% / 90.9% / 80.3% (0.853)\n",
      "      Url3             KNN. Train set size   600. 83.8% / 85.5% / 81.3% (0.833)\n",
      "      Url3       Bernoulli. Train set size   600. 71.9% / 91.7% / 48.0% (0.631)\n",
      "      Url3             SGD. Train set size   600. 86.5% / 89.8% / 82.3% (0.859)\n",
      "      Url3    RandomForest. Train set size   600. 82.6% / 94.0% / 69.6% (0.800)\n",
      "      Url3       LinearSVC. Train set size   600. 86.6% / 89.9% / 82.6% (0.861)\n",
      "      Url6             KNN. Train set size   600. 85.1% / 89.8% / 79.2% (0.842)\n",
      "      Url6       Bernoulli. Train set size   600. 71.5% / 97.1% / 44.4% (0.609)\n",
      "      Url6             SGD. Train set size   600. 87.4% / 91.7% / 82.2% (0.867)\n",
      "      Url6    RandomForest. Train set size   600. 81.2% / 94.7% / 66.2% (0.779)\n",
      "      Url6       LinearSVC. Train set size   600. 87.6% / 91.8% / 82.6% (0.869)\n",
      "     Url12             KNN. Train set size   600. 85.7% / 93.2% / 76.9% (0.843)\n",
      "     Url12       Bernoulli. Train set size   600. 70.2% / 99.5% / 40.6% (0.576)\n",
      "     Url12             SGD. Train set size   600. 87.8% / 92.8% / 82.1% (0.871)\n",
      "     Url12    RandomForest. Train set size   600. 79.7% / 98.5% / 60.3% (0.748)\n",
      "     Url12       LinearSVC. Train set size   600. 88.0% / 93.1% / 82.0% (0.872)\n",
      "      Url3             KNN. Train set size  1200. 86.5% / 89.5% / 82.7% (0.859)\n",
      "      Url3       Bernoulli. Train set size  1200. 72.3% / 91.8% / 48.9% (0.638)\n",
      "      Url3             SGD. Train set size  1200. 88.9% / 92.2% / 85.0% (0.885)\n",
      "      Url3    RandomForest. Train set size  1200. 82.4% / 95.2% / 68.1% (0.794)\n",
      "      Url3       LinearSVC. Train set size  1200. 89.4% / 92.4% / 85.8% (0.890)\n",
      "      Url6             KNN. Train set size  1200. 88.5% / 92.9% / 83.4% (0.879)\n",
      "      Url6       Bernoulli. Train set size  1200. 73.2% / 96.4% / 48.3% (0.643)\n",
      "      Url6             SGD. Train set size  1200. 89.6% / 93.3% / 85.4% (0.892)\n",
      "      Url6    RandomForest. Train set size  1200. 81.0% / 97.4% / 63.7% (0.770)\n",
      "      Url6       LinearSVC. Train set size  1200. 90.1% / 93.5% / 86.2% (0.897)\n",
      "     Url12             KNN. Train set size  1200. 88.5% / 94.8% / 81.4% (0.876)\n",
      "     Url12       Bernoulli. Train set size  1200. 72.7% / 99.2% / 45.9% (0.627)\n",
      "     Url12             SGD. Train set size  1200. 89.3% / 93.8% / 84.2% (0.888)\n",
      "     Url12    RandomForest. Train set size  1200. 79.4% / 99.0% / 59.4% (0.742)\n",
      "     Url12       LinearSVC. Train set size  1200. 89.9% / 94.3% / 85.1% (0.894)\n",
      "      Url3             KNN. Train set size  2400. 88.1% / 92.3% / 83.1% (0.874)\n",
      "      Url3       Bernoulli. Train set size  2400. 72.4% / 91.7% / 49.2% (0.640)\n",
      "      Url3             SGD. Train set size  2400. 89.7% / 92.9% / 86.0% (0.893)\n",
      "      Url3    RandomForest. Train set size  2400. 81.9% / 96.7% / 66.1% (0.785)\n",
      "      Url3       LinearSVC. Train set size  2400. 91.1% / 94.0% / 87.9% (0.908)\n",
      "      Url6             KNN. Train set size  2400. 90.2% / 95.0% / 84.9% (0.897)\n",
      "      Url6       Bernoulli. Train set size  2400. 73.4% / 96.4% / 48.6% (0.646)\n",
      "      Url6             SGD. Train set size  2400. 91.2% / 94.4% / 87.5% (0.908)\n",
      "      Url6    RandomForest. Train set size  2400. 79.0% / 99.0% / 58.6% (0.736)\n",
      "      Url6       LinearSVC. Train set size  2400. 92.7% / 95.9% / 89.2% (0.924)\n",
      "     Url12             KNN. Train set size  2400. 89.4% / 95.0% / 83.1% (0.887)\n",
      "     Url12       Bernoulli. Train set size  2400. 73.0% / 99.2% / 46.4% (0.632)\n",
      "     Url12             SGD. Train set size  2400. 91.1% / 95.3% / 86.4% (0.906)\n",
      "     Url12    RandomForest. Train set size  2400. 77.1% / 99.5% / 54.6% (0.705)\n",
      "     Url12       LinearSVC. Train set size  2400. 92.2% / 95.5% / 88.6% (0.919)\n",
      "      Url3             KNN. Train set size  4800. 90.6% / 94.1% / 86.6% (0.902)\n",
      "      Url3       Bernoulli. Train set size  4800. 73.2% / 91.8% / 50.9% (0.655)\n",
      "      Url3             SGD. Train set size  4800. 91.0% / 94.1% / 87.6% (0.907)\n",
      "      Url3    RandomForest. Train set size  4800. 81.4% / 96.9% / 64.8% (0.776)\n",
      "      Url3       LinearSVC. Train set size  4800. 92.6% / 94.4% / 90.5% (0.924)\n",
      "      Url6             KNN. Train set size  4800. 91.4% / 94.5% / 87.9% (0.911)\n",
      "      Url6       Bernoulli. Train set size  4800. 74.6% / 97.5% / 50.5% (0.665)\n",
      "      Url6             SGD. Train set size  4800. 91.9% / 95.2% / 88.2% (0.916)\n",
      "      Url6    RandomForest. Train set size  4800. 76.2% / 98.6% / 53.1% (0.690)\n",
      "      Url6       LinearSVC. Train set size  4800. 94.1% / 95.7% / 92.3% (0.940)\n",
      "     Url12             KNN. Train set size  4800. 91.0% / 94.5% / 87.1% (0.907)\n",
      "     Url12       Bernoulli. Train set size  4800. 74.1% / 98.9% / 48.7% (0.653)\n",
      "     Url12             SGD. Train set size  4800. 92.1% / 96.1% / 87.8% (0.918)\n",
      "     Url12    RandomForest. Train set size  4800. 73.8% / 99.9% / 47.7% (0.645)\n",
      "     Url12       LinearSVC. Train set size  4800. 94.2% / 96.1% / 92.1% (0.941)\n",
      "      Url3             KNN. Train set size  9600. 91.5% / 95.1% / 87.5% (0.911)\n",
      "      Url3       Bernoulli. Train set size  9600. 73.4% / 91.8% / 51.3% (0.658)\n",
      "      Url3             SGD. Train set size  9600. 91.5% / 94.3% / 88.4% (0.912)\n",
      "      Url3    RandomForest. Train set size  9600. 81.3% / 97.5% / 64.2% (0.774)\n",
      "      Url3       LinearSVC. Train set size  9600. 93.8% / 95.6% / 91.9% (0.937)\n",
      "      Url6             KNN. Train set size  9600. 92.1% / 95.2% / 88.7% (0.918)\n",
      "      Url6       Bernoulli. Train set size  9600. 75.1% / 97.8% / 51.4% (0.674)\n",
      "      Url6             SGD. Train set size  9600. 92.3% / 95.2% / 89.0% (0.920)\n",
      "      Url6    RandomForest. Train set size  9600. 78.6% / 98.6% / 58.1% (0.731)\n",
      "      Url6       LinearSVC. Train set size  9600. 95.4% / 97.1% / 93.6% (0.953)\n",
      "     Url12             KNN. Train set size  9600. 92.3% / 95.6% / 88.8% (0.921)\n",
      "     Url12       Bernoulli. Train set size  9600. 74.7% / 99.0% / 50.0% (0.664)\n",
      "     Url12             SGD. Train set size  9600. 92.6% / 96.2% / 88.7% (0.923)\n",
      "     Url12    RandomForest. Train set size  9600. 76.6% / 99.9% / 53.2% (0.695)\n",
      "     Url12       LinearSVC. Train set size  9600. 94.9% / 96.8% / 92.9% (0.948)\n",
      "      Url3             KNN. Train set size 19200. 92.0% / 95.5% / 88.1% (0.917)\n",
      "      Url3       Bernoulli. Train set size 19200. 73.2% / 91.7% / 50.9% (0.655)\n",
      "      Url3             SGD. Train set size 19200. 91.7% / 94.4% / 88.7% (0.914)\n",
      "      Url3    RandomForest. Train set size 19200. 80.5% / 97.4% / 62.6% (0.762)\n",
      "      Url3       LinearSVC. Train set size 19200. 95.3% / 96.8% / 93.8% (0.953)\n",
      "      Url6             KNN. Train set size 19200. 92.8% / 95.5% / 89.8% (0.926)\n",
      "      Url6       Bernoulli. Train set size 19200. 75.3% / 97.4% / 52.0% (0.678)\n",
      "      Url6             SGD. Train set size 19200. 92.5% / 95.7% / 89.0% (0.923)\n",
      "      Url6    RandomForest. Train set size 19200. 73.9% / 98.4% / 48.5% (0.650)\n",
      "      Url6       LinearSVC. Train set size 19200. 96.5% / 97.8% / 95.2% (0.965)\n",
      "     Url12             KNN. Train set size 19200. 93.7% / 96.6% / 90.6% (0.935)\n",
      "     Url12       Bernoulli. Train set size 19200. 75.3% / 99.0% / 51.1% (0.674)\n",
      "     Url12             SGD. Train set size 19200. 93.2% / 96.3% / 89.7% (0.929)\n",
      "     Url12    RandomForest. Train set size 19200. 77.7% / 99.9% / 55.5% (0.713)\n",
      "     Url12       LinearSVC. Train set size 19200. 96.5% / 97.7% / 95.3% (0.965)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    for model_type in [\"Url3\", \"Url6\", \"Url12\"]:\n",
    "        dataset = load_model(model_type, train_size)\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   KNeighborsClassifier(2), \"KNN\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   BernoulliNB(), \"Bernoulli\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "        test_model(dataset, model_type, train_size,\n",
    "                   LinearSVC(), \"LinearSVC\",\n",
    "                   train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/url_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FileSize             KNN. Train set size 19200. 52.5% / 58.2% / 17.6% (0.204)\n",
      "  FileSize       Bernoulli. Train set size 19200. 61.8% / 61.1% / 64.9% (0.630)\n",
      "  FileSize             SGD. Train set size 19200. 66.0% / 64.0% / 73.4% (0.683)\n",
      "  FileSize    RandomForest. Train set size 19200. 66.0% / 64.0% / 73.4% (0.683)\n",
      "  FileSize       LinearSVC. Train set size 19200. 66.0% / 64.0% / 73.4% (0.683)\n"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "model_type = \"FileSize\"\n",
    "dataset = load_model(model_type, train_size)\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           KNeighborsClassifier(2), \"KNN\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           BernoulliNB(), \"Bernoulli\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           linear_model.SGDClassifier(n_iter=1000, loss=\"log\"), \"SGD\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           RandomForestClassifier(max_depth=15, n_estimators=100, max_features=15), \"RandomForest\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "\n",
    "test_model(dataset, model_type, train_size,\n",
    "           LinearSVC(), \"LinearSVC\",\n",
    "           train_size == TRAIN_SIZES[-1])\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/filesize_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ('BiRegEx', 'Url6', 'FileSize'): 500000 + 500000 + 15 = 1000015\n",
      "Labels equal: [True, True] [True, True]\n",
      "BiRegEx-Url6-FileSize    RandomForest. Train set size 19200. 81.9% / 93.4% / 68.6% (0.791)\n",
      "BiRegEx-Url6-FileSize       LinearSVC. Train set size 19200. 96.6% / 97.8% / 95.4% (0.966)\n",
      "Datasets ('BiRegEx1K', 'Url6', 'FileSize'): 101468 + 500000 + 15 = 601483\n",
      "Labels equal: [True, True] [True, True]\n",
      "BiRegEx1K-Url6-FileSize    RandomForest. Train set size 19200. 78.7% / 98.5% / 58.4% (0.733)\n",
      "BiRegEx1K-Url6-FileSize       LinearSVC. Train set size 19200. 96.4% / 97.5% / 95.3% (0.964)\n",
      "Datasets ('BiRegEx', 'Url6'): 500000 + 500000 = 1000000\n",
      "Labels equal: [True] [True]\n",
      "BiRegEx-Url6    RandomForest. Train set size 19200. 81.5% / 94.5% / 67.0% (0.784)\n",
      "BiRegEx-Url6       LinearSVC. Train set size 19200. 96.5% / 98.0% / 95.0% (0.965)\n",
      "Datasets ('BiRegEx1K', 'Url6'): 101468 + 500000 = 601468\n",
      "Labels equal: [True] [True]\n",
      "BiRegEx1K-Url6    RandomForest. Train set size 19200. 79.0% / 98.7% / 58.8% (0.737)\n",
      "BiRegEx1K-Url6       LinearSVC. Train set size 19200. 96.3% / 97.6% / 94.8% (0.962)\n",
      "Datasets ('BiRegEx', 'TriAST', 'Url6'): 500000 + 500000 + 500000 = 1500000\n",
      "Labels equal: [True, True] [True, True]\n",
      "BiRegEx-TriAST-Url6    RandomForest. Train set size 19200. 83.0% / 92.0% / 72.4% (0.810)\n",
      "BiRegEx-TriAST-Url6       LinearSVC. Train set size 19200. 96.2% / 97.6% / 94.6% (0.961)\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/usr/src/app/model-data/dataset_Random2Vec_19200.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5e567719ce85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         (\"Word2Vec\", \"AST2Vec\") ]:\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     test_model(dataset, model_type, train_size,\n",
      "\u001b[0;32m<ipython-input-3-9571cb292cdb>\u001b[0m in \u001b[0;36mconcat_models\u001b[0;34m(model_names, train_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconcat_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     X_trains = [\n",
      "\u001b[0;32m<ipython-input-3-9571cb292cdb>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_type, train_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/model-data/dataset_%s_%d.pickle\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mREPO_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconcat_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/usr/src/app/model-data/dataset_Random2Vec_19200.pickle'"
     ]
    }
   ],
   "source": [
    "output_table = []\n",
    "\n",
    "train_size = TRAIN_SIZES[-1]\n",
    "for model_names in [\n",
    "        (\"BiRegEx\", \"Url6\", \"FileSize\"),\n",
    "        (\"BiRegEx1K\", \"Url6\", \"FileSize\"),\n",
    "        (\"BiRegEx\", \"Url6\"),\n",
    "        (\"BiRegEx1K\", \"Url6\"),\n",
    "        (\"BiRegEx\", \"TriAST\", \"Url6\"),\n",
    "        (\"RegEx\", \"Random2Vec\"),\n",
    "        (\"RegEx\", \"AST\"),\n",
    "        (\"BiRegEx\", \"Word2Vec\"),\n",
    "        (\"BiRegEx\", \"TriAST\"),\n",
    "        (\"Word2Vec\", \"AST2Vec\") ]:\n",
    "    model_type = \"-\".join(model_names)\n",
    "    dataset = concat_models(model_names, train_size)\n",
    "\n",
    "    test_model(dataset, model_type, train_size,\n",
    "               RandomForestClassifier(max_depth=15, n_estimators=100, max_features=30), \"RandomForest\",\n",
    "               False)\n",
    "\n",
    "    test_model(dataset, model_type, train_size,\n",
    "               LinearSVC(), \"LinearSVC\",\n",
    "               False)\n",
    "        \n",
    "output = (\"Model Type,Model,Training set,Accuracy,Precision,Recall,F1 score\\n\" +\n",
    "        \"\\n\".join([\",\".join([str(s) for s in row]) for row in output_table]))\n",
    "with open(\"%s/results/combined_models.csv\" % REPO_ROOT, \"w\") as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(X_test, Y_test, b, w):\n",
    "    nrows = np.shape(X_test)[0]\n",
    "    num_correct = 0\n",
    "    for row in xrange(nrows):\n",
    "        score = b + np.dot(X_test.getrow(row).toarray().flatten(), w)\n",
    "        pred = 1 if score > 0 else 0\n",
    "        actual = Y_test[row]\n",
    "        correct = (pred == actual)\n",
    "        if correct:\n",
    "            num_correct += 1\n",
    "        \n",
    "    return float(num_correct) / nrows\n",
    "\n",
    "def test_thresholds(dataset, b, w):\n",
    "    max_threshold = np.max(np.abs(w))\n",
    "    best_threshold = 0\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    if isinstance(dataset[\"X_test\"], np.ndarray):\n",
    "        X_test_flat = dataset[\"X_test\"]\n",
    "    else:\n",
    "        X_test_flat = dataset[\"X_test\"].tocsc()\n",
    "        \n",
    "    print \"Max weight: %f\" % max_threshold\n",
    "    for threshold in np.arange(0, max_threshold, max_threshold/100):\n",
    "        wprime = np.array(w)\n",
    "        wprime[np.abs(wprime) <= threshold] = 0\n",
    "        nonzero_count = np.sum(wprime != 0)\n",
    "        accuracy = calculate_accuracy(X_test_flat, dataset[\"Y_test\"], b, wprime)\n",
    "        best_accuracy = max(best_accuracy, accuracy)\n",
    "        print \"Accuracy at threshold %f (%d): %.1f%%\" % (threshold, nonzero_count, accuracy * 100)\n",
    "        if accuracy > 0.99 * best_accuracy:\n",
    "            best_threshold = threshold\n",
    "        else:\n",
    "            break\n",
    "    print \"Done. Best threshold: %f\" % best_threshold\n",
    "    return best_threshold\n",
    "\n",
    "def recover_input(tokenized_input, test_vector, vocab_info):\n",
    "    vocab_size = np.shape(vocab_info[\"idf\"])[0]\n",
    "    input_vector = np.zeros(vocab_size)\n",
    "\n",
    "    for tokens in tokenized_input:\n",
    "        #print \"%s -> %s\" % (tokens, vocab_info[\"vocab\"].get(tokens, \"-\"))\n",
    "        if tokens in vocab_info[\"vocab\"]:\n",
    "            index = vocab_info[\"vocab\"][tokens]\n",
    "            idf = vocab_info[\"idf\"][index]\n",
    "            input_vector[index] += idf\n",
    "\n",
    "    print \"norm: %s\" % np.linalg.norm(input_vector)\n",
    "    input_vector = input_vector / np.linalg.norm(input_vector)\n",
    "\n",
    "    print \"Test vector: %s\" % test_vector[(test_vector != 0) | (input_vector != 0)]\n",
    "    print \"Input vector: %s\" % input_vector[(test_vector != 0) | (input_vector != 0)]\n",
    "    \n",
    "    for idx in xrange(np.shape(test_vector)[0]):\n",
    "        if test_vector[idx] != 0 and input_vector[idx] == 0:\n",
    "            print \"Test %d %s = %f\" % (\n",
    "                idx, next(v for v, i in vocab_info[\"vocab\"].iteritems() if i == idx), test_vector[idx])\n",
    "        if test_vector[idx] == 0 and input_vector[idx] != 0:\n",
    "            print \"Input %d %s = %f\" % (\n",
    "                idx, next(v for v, i in vocab_info[\"vocab\"].iteritems() if i == idx), input_vector[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 266.414786012\n",
      "Test vector: [ 0.2211524   0.01987661  0.03141249 ...,  0.01292275  0.03264087\n",
      "  0.05653225]\n",
      "Input vector: [ 0.2211524   0.01987661  0.03141249 ...,  0.01292275  0.03264087\n",
      "  0.05653225]\n"
     ]
    }
   ],
   "source": [
    "def validate_url6():\n",
    "    model_type = \"Url6\"\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        vocab_info = pickle.load(f)\n",
    "\n",
    "    # Try to recover a test input\n",
    "    url = dataset[\"urls_test\"][0]\n",
    "    url_chars = util.ngramizer(util.tokenize_url, 6)(next(util.parse_url([{\"url\": url}])))\n",
    "    test_vector = dataset[\"X_test\"].getrow(0).toarray().flatten()\n",
    "    recover_input(url_chars, test_vector, vocab_info)\n",
    "\n",
    "validate_url6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm: 29.5626283938\n",
      "Test vector: [ 0.23359267  0.05684434  0.13917522  0.14201405  0.14218708  0.13108535\n",
      "  0.0351596   0.07040361  0.03932155  0.03858764  0.06079984  0.0753506\n",
      "  0.14037187  0.14218708  0.09358728  0.14218708  0.14218708  0.08745182\n",
      "  0.14214374  0.09340654  0.14086872  0.07828346  0.06077638  0.13331221\n",
      "  0.07932212  0.05829805  0.06432907  0.04666207  0.13491761  0.14197093\n",
      "  0.14020786  0.14028977  0.11483334  0.14218708  0.14066081  0.14218708\n",
      "  0.13837681  0.14218708  0.13776246  0.14053668  0.14218708  0.14218708\n",
      "  0.14218708  0.14218708  0.14188486  0.14214374  0.26349541  0.14218708\n",
      "  0.14218708  0.14171337  0.14201405  0.14218708  0.14218708  0.14201405\n",
      "  0.14205722  0.12961839  0.13105413  0.07545332  0.06016067  0.14218708\n",
      "  0.03735115  0.07142665  0.04105859  0.06735573]\n",
      "Input vector: [ 0.23359267  0.05684434  0.13917522  0.14201405  0.14218708  0.13108535\n",
      "  0.0351596   0.07040361  0.03932155  0.03858764  0.06079984  0.0753506\n",
      "  0.14037187  0.14218708  0.09358728  0.14218708  0.14218708  0.08745182\n",
      "  0.14214374  0.09340654  0.14086872  0.07828346  0.06077638  0.13331221\n",
      "  0.07932212  0.05829805  0.06432907  0.04666207  0.13491761  0.14197093\n",
      "  0.14020786  0.14028977  0.11483334  0.14218708  0.14066081  0.14218708\n",
      "  0.13837681  0.14218708  0.13776246  0.14053668  0.14218708  0.14218708\n",
      "  0.14218708  0.14218708  0.14188486  0.14214374  0.26349541  0.14218708\n",
      "  0.14218708  0.14171337  0.14201405  0.14218708  0.14218708  0.14201405\n",
      "  0.14205722  0.12961839  0.13105413  0.07545332  0.06016067  0.14218708\n",
      "  0.03735115  0.07142665  0.04105859  0.06735573]\n"
     ]
    }
   ],
   "source": [
    "def validate_bigregex():\n",
    "    model_type = \"BiRegEx1K\"\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "    dataset = load_model(model_type, train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, model_type, train_size), \"r\") as f:\n",
    "        vocab_info = pickle.load(f)\n",
    "\n",
    "    # Try to recover a test input\n",
    "    sha = dataset[\"shas_test\"][0]\n",
    "    tokens = util.ngramizer(util.tokenize_js, 2)(next(util.parse_js([{\"sha\": sha}])))\n",
    "    test_vector = dataset[\"X_test\"].getrow(0).toarray().flatten()\n",
    "    recover_input(tokens, test_vector, vocab_info)\n",
    "\n",
    "validate_bigregex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max weight: 3.980340\n",
      "Accuracy at threshold 0.000000 (416002): 96.5%\n",
      "Accuracy at threshold 0.039803 (127899): 96.5%\n",
      "Accuracy at threshold 0.079607 (58556): 96.5%\n",
      "Accuracy at threshold 0.119410 (30121): 96.3%\n",
      "Accuracy at threshold 0.159214 (16497): 96.0%\n",
      "Accuracy at threshold 0.199017 (9596): 95.5%\n",
      "Done. Best threshold: 0.159214\n",
      "16497 indices\n"
     ]
    }
   ],
   "source": [
    "def create_url_model():\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "    dataset = load_model(\"Url6\", train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, \"Url6\", train_size), \"r\") as f:\n",
    "        url_vocab_info = pickle.load(f)\n",
    "\n",
    "    model = LinearSVC()\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "\n",
    "    b = model.intercept_[0]\n",
    "    w = model.coef_.flatten()\n",
    "\n",
    "    best_threshold = test_thresholds(dataset, b, w)\n",
    "\n",
    "    # Reduce vocabulary using the threshold\n",
    "    vocab_indices = np.nonzero(np.abs(w) > best_threshold)[0].tolist()\n",
    "\n",
    "    index_map = {\n",
    "        vocab_indices[idx]: idx\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "    }\n",
    "\n",
    "    print \"%d indices\" % len(vocab_indices)\n",
    "\n",
    "    url_model = {\n",
    "        \"vocab\": {\n",
    "            key: index_map[index]\n",
    "            for key, index in url_vocab_info[\"vocab\"].iteritems()\n",
    "            if index in index_map\n",
    "        },\n",
    "        \"idf\": [url_vocab_info[\"idf\"][idx] for idx in vocab_indices],\n",
    "        \"w\": [w[idx] for idx in vocab_indices],\n",
    "        \"b\": b,\n",
    "    }\n",
    "\n",
    "    with open(\"%s/model-data/url_model.json\" % REPO_ROOT, \"w\") as f:\n",
    "        json.dump(url_model, f)\n",
    "    \n",
    "create_url_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ['Url6', 'BiRegEx1K', 'FileSize']: 500000 + 101468 + 15 = 601483\n",
      "Labels equal: [True, True] [True, True]\n",
      "Max weight: 3.180620\n",
      "Accuracy at threshold 0.000000 (506054): 96.6%\n",
      "Accuracy at threshold 0.031806 (141096): 96.5%\n",
      "Accuracy at threshold 0.063612 (65634): 96.4%\n",
      "Accuracy at threshold 0.095419 (34526): 96.3%\n",
      "Accuracy at threshold 0.127225 (19823): 96.1%\n",
      "Accuracy at threshold 0.159031 (11937): 95.8%\n",
      "Accuracy at threshold 0.190837 (7590): 95.4%\n",
      "Done. Best threshold: 0.159031\n",
      "11937 indices (7712 url; 4216 script; 9 size)\n"
     ]
    }
   ],
   "source": [
    "def create_final_model():\n",
    "    train_size = TRAIN_SIZES[-1]\n",
    "\n",
    "    dataset = concat_models([\"Url6\", \"BiRegEx1K\", \"FileSize\"], train_size)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, \"Url6\", train_size), \"r\") as f:\n",
    "        url_vocab_info = pickle.load(f)\n",
    "\n",
    "    with open(\"%s/model-data/vocab_%s_%d.pickle\" % (REPO_ROOT, \"BiRegEx1K\", train_size), \"r\") as f:\n",
    "        script_vocab_info = pickle.load(f)\n",
    "\n",
    "    size_vocab = {\n",
    "        2**(n+5): n for n in xrange(0, 15)\n",
    "    }\n",
    "    \n",
    "    model = LinearSVC()\n",
    "    model.fit(dataset[\"X_train\"], dataset[\"Y_train\"])\n",
    "\n",
    "    b = model.intercept_[0]\n",
    "    w = model.coef_.flatten()\n",
    "\n",
    "    best_threshold = test_thresholds(dataset, b, w)\n",
    "\n",
    "    # Reduce vocabulary using the threshold\n",
    "    vocab_indices = np.nonzero(np.abs(w) > best_threshold)[0].tolist()\n",
    "\n",
    "    url_vocab_size = np.shape(url_vocab_info[\"idf\"])[0]\n",
    "    script_vocab_size = np.shape(script_vocab_info[\"idf\"])[0]\n",
    "\n",
    "    url_index_map = {\n",
    "        vocab_indices[idx]: idx\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "        if vocab_indices[idx] < url_vocab_size\n",
    "    }\n",
    "\n",
    "    url_new_vocab_size = len(url_index_map)\n",
    "    \n",
    "    script_index_map = {\n",
    "        (vocab_indices[idx] - url_vocab_size): idx - url_new_vocab_size\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "        if vocab_indices[idx] >= url_vocab_size and vocab_indices[idx] < url_vocab_size + script_vocab_size\n",
    "    }\n",
    "    \n",
    "    script_new_vocab_size = len(script_index_map)\n",
    "    \n",
    "    size_index_map = {\n",
    "        (vocab_indices[idx] - url_vocab_size - script_vocab_size): idx - url_new_vocab_size - script_new_vocab_size\n",
    "        for idx in xrange(len(vocab_indices))\n",
    "        if vocab_indices[idx] >= url_vocab_size + script_vocab_size\n",
    "    }\n",
    "\n",
    "    print \"%d indices (%d url; %d script; %d size)\" % (\n",
    "        len(vocab_indices), len(url_index_map), len(script_index_map), len(size_index_map))\n",
    "\n",
    "    final_model = {\n",
    "        # Just the vocab & weights for URL features\n",
    "        \"url\": {\n",
    "            \"vocab\": {\n",
    "                key: url_index_map[index]\n",
    "                for key, index in url_vocab_info[\"vocab\"].iteritems()\n",
    "                if index in url_index_map\n",
    "            },\n",
    "            \"idf\": [url_vocab_info[\"idf\"][idx] for idx in vocab_indices[:url_new_vocab_size]],\n",
    "            \"w\": [w[idx] for idx in vocab_indices[:url_new_vocab_size]],\n",
    "        },\n",
    "        # Just the vocab & weights for the script features\n",
    "        \"script\": {\n",
    "             \"vocab\": {\n",
    "                key: script_index_map[index]\n",
    "                for key, index in script_vocab_info[\"vocab\"].iteritems()\n",
    "                if index in script_index_map\n",
    "            },\n",
    "            \"idf\": [script_vocab_info[\"idf\"][idx - url_vocab_size] for idx in vocab_indices[url_new_vocab_size:url_new_vocab_size + script_new_vocab_size]],\n",
    "            \"w\": [w[idx] for idx in vocab_indices[url_new_vocab_size:url_new_vocab_size + script_new_vocab_size]],\n",
    "        },\n",
    "        # Just the indices & weights for the file size features\n",
    "        \"size\": {\n",
    "            \"vocab\": {\n",
    "                key: size_index_map[index]\n",
    "                for key, index in size_vocab.iteritems()\n",
    "                if index in size_index_map\n",
    "            },\n",
    "            \"idf\": [1 for idx in vocab_indices[url_new_vocab_size + script_new_vocab_size:]],\n",
    "            \"w\": [w[idx] for idx in vocab_indices[url_new_vocab_size + script_new_vocab_size:]],\n",
    "        },\n",
    "        # The SVM intercept\n",
    "        \"b\": b,\n",
    "    }\n",
    "\n",
    "    with open(\"%s/model-data/final_model.json\" % REPO_ROOT, \"w\") as f:\n",
    "        json.dump(final_model, f)\n",
    "\n",
    "    \n",
    "create_final_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
