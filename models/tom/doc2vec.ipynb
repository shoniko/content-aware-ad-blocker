{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'util' from 'util.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "REPO_ROOT = \"/usr/src/app\"\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "\n",
    "import util\n",
    "reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/usr/src/app/model-data/metadata.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3a3016e364c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/model-data/metadata.pickle\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mREPO_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msize_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTRAIN_SIZES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_sizes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTEST_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/usr/src/app/model-data/metadata.pickle'"
     ]
    }
   ],
   "source": [
    "with open(\"%s/model-data/metadata.pickle\" % (REPO_ROOT,), \"r\") as f:\n",
    "    size_data = pickle.load(f)\n",
    "    \n",
    "TRAIN_SIZES = size_data[\"train_sizes\"]\n",
    "TEST_SIZE = size_data[\"test_size\"]\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "VOCAB_SIZE = 10000\n",
    "WORD_EMBEDDING_SIZE = 1600\n",
    "DOC_EMBEDDING_SIZE = 1000\n",
    "CONTEXT_SIZE = 3\n",
    "\n",
    "VALIDATE_SIZE = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"%s/scripts/table_balanced.json\" % REPO_ROOT) as f:\n",
    "    data_table = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = collections.Counter()\n",
    "print \"Tokenizing %d scripts...\" % len(data_table)\n",
    "for idx, script in enumerate(data_table):\n",
    "    if idx % 1000 == 0:\n",
    "        print \"%d done.\" % idx\n",
    "        \n",
    "    with open(\"%s/scripts/%s.js\" % (REPO_ROOT, script[\"sha\"])) as f:\n",
    "        js = f.read()\n",
    "        tokens = util.tokenize_js(js)\n",
    "        counts.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for token, count in counts.most_common(100):\n",
    "    print \"%s: %d\" % (token, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f346cdaf4239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word_table = {\n\u001b[1;32m      2\u001b[0m     \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counts' is not defined"
     ]
    }
   ],
   "source": [
    "word_table = {\n",
    "    entry[0]: idx\n",
    "    for idx, entry in enumerate(counts.most_common(VOCAB_SIZE-1))\n",
    "}\n",
    "\n",
    "with open(\"%s/model-data/js-vocab.pickle\" % (REPO_ROOT,), \"w\") as f:\n",
    "    pickle.dump(word_table, f)\n",
    "\n",
    "def numerize(word):\n",
    "    return word_table.get(word, VOCAB_SIZE-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_lookup(embeddings, table_rows):\n",
    "    vocab_size, embed_size = np.shape(embeddings)\n",
    "    embeddings_trans = np.transpose(embeddings)\n",
    "\n",
    "    ret = np.zeros([len(table_rows), embed_size])\n",
    "    for script_idx, script in enumerate(util.parse_js(table_rows)):\n",
    "        token_ids = [\n",
    "            numerize(token)\n",
    "            for token in util.tokenize_js(script)\n",
    "        ]\n",
    "        word_vec = np.zeros(vocab_size)\n",
    "        for token_id in token_ids:\n",
    "            word_vec[token_id] += 1\n",
    "\n",
    "        ret[script_idx] = np.matmul(embeddings_trans, word_vec)\n",
    "\n",
    "    return ret\n",
    "\n",
    "def save_training_set(embeddings, data_table, train_indices, test_indices):   \n",
    "    data = {\n",
    "        \"X_train\": embedding_lookup(embeddings, [data_table[index] for index in train_indices]),\n",
    "        \"Y_train\": np.array([data_table[index][\"flag-any\"] for index in train_indices]),\n",
    "        \"X_test\": embedding_lookup(embeddings, [data_table[index] for index in test_indices]),\n",
    "        \"Y_test\": np.array([data_table[index][\"flag-any\"] for index in test_indices]),\n",
    "    }\n",
    "\n",
    "    with open(\"%s/model-data/dataset_Word2Vec_%d.pickle\" % (REPO_ROOT, len(train_indices)), \"w\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Word2Vec algorithm\n",
    "class WordEmbeddingGraph(object):\n",
    "    def __init__(self, data_table, batch_size, vocabulary_size, embedding_size, context_size,\n",
    "                 train_size, test_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.context_size = context_size\n",
    "        self.train_size = train_size\n",
    "        \n",
    "        random.seed(9812)\n",
    "        indices = random.sample(range(len(data_table)), train_size + test_size)\n",
    "        self.train_indices = indices[:train_size]\n",
    "        self.validate_indices = indices[-test_size:]\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():    \n",
    "            self.embeddings = tf.Variable(\n",
    "                tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"word_embeddings\")\n",
    "\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings), 1, keep_dims=True))\n",
    "            self.normalized_embeddings = self.embeddings / norm\n",
    "\n",
    "            nce_weights = tf.Variable(\n",
    "              tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                  stddev=1.0 / math.sqrt(embedding_size)))\n",
    "            nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "            self.x_ph = tf.placeholder(tf.int32, shape=[batch_size,CONTEXT_SIZE])\n",
    "            self.y_ph = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "\n",
    "            embed = tf.add_n(\n",
    "                [tf.nn.embedding_lookup(self.embeddings, self.x_ph[:,idx]) for idx in xrange(CONTEXT_SIZE)])\n",
    "\n",
    "            # Compute the NCE loss, using a sample of the negative labels each time.\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(nce_weights, nce_biases, embed, self.y_ph,\n",
    "                               64, vocabulary_size))\n",
    "\n",
    "            output = tf.transpose(tf.matmul(nce_weights, tf.transpose(embed))) + nce_biases\n",
    "            self.y_pred = tf.argmax(tf.nn.softmax(output), 1)\n",
    "\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(self.loss)\n",
    "            \n",
    "    def generate_batch(self, indices, sample_ratio):\n",
    "        x = np.ndarray(shape=(self.batch_size, self.context_size), dtype=np.int32)\n",
    "        y = np.ndarray(shape=(self.batch_size,1), dtype=np.int32)\n",
    "        batch_idx = 0\n",
    "        for index in indices:\n",
    "            with open(\"%s/scripts/%s.js\" % (REPO_ROOT, data_table[index][\"sha\"])) as f:\n",
    "                js = f.read()\n",
    "                tokens = [numerize(token) for token in util.tokenize_js(js)]\n",
    "                if len(tokens) < self.context_size + 1:\n",
    "                    continue\n",
    "\n",
    "                sample_size = len(tokens) - 1 - self.context_size\n",
    "                num_to_sample = max(1, int(math.floor(sample_size * sample_ratio)))\n",
    "                for pos in random.sample(range(0, sample_size + 1), num_to_sample):\n",
    "                    for idx in xrange(self.context_size):\n",
    "                        x[batch_idx,idx] = tokens[pos+idx]\n",
    "                    y[batch_idx,0] = tokens[pos + self.context_size]\n",
    "                    batch_idx += 1\n",
    "                    if batch_idx == self.batch_size:\n",
    "                        yield {self.x_ph: x, self.y_ph: y}\n",
    "                        batch_idx = 0\n",
    "                        \n",
    "    def train(self, session):\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        batch_idx = 1\n",
    "        average_loss = 0\n",
    "        for feed_dict in self.generate_batch(self.train_indices, 1.0):\n",
    "            _, cur_loss = session.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += cur_loss\n",
    "\n",
    "            if batch_idx % 2000 == 0:\n",
    "                validate_batches = 0\n",
    "                average_accuracy = 0\n",
    "                for feed_dict2 in self.generate_batch(self.validate_indices, 0.01):\n",
    "                    predicted_labels = session.run([self.y_pred], feed_dict=feed_dict2)\n",
    "                    predicted_labels = np.transpose(predicted_labels)\n",
    "                    average_accuracy += (\n",
    "                        float(np.count_nonzero(feed_dict2[self.y_ph] == predicted_labels)) / BATCH_SIZE)\n",
    "                    validate_batches += 1\n",
    "\n",
    "                print \"%d Loss: %f, Accuracy: %f\" % (\n",
    "                    batch_idx,\n",
    "                    average_loss / 1999,\n",
    "                    average_accuracy / validate_batches)\n",
    "                average_loss = 0\n",
    "\n",
    "            batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Training size 300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-26d3aafecd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msave_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-68556d66f08b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0maverage_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfeed_dict2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     average_accuracy += (\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for train_size in TRAIN_SIZES:\n",
    "    print \"Word2Vec Training size %d\" % train_size\n",
    "    \n",
    "    g = WordEmbeddingGraph(\n",
    "        data_table, BATCH_SIZE, VOCAB_SIZE, WORD_EMBEDDING_SIZE, CONTEXT_SIZE, train_size, TEST_SIZE)\n",
    "\n",
    "    with tf.Session(graph=g.graph) as session:\n",
    "        g.train(session)\n",
    "        save_training_set(g.normalized_embeddings.eval(), data_table, g.train_indices, g.validate_indices)\n",
    "    \n",
    "print \"Done training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8fffd3979e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'final_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "model = TSNE(n_components=2, random_state=0)\n",
    "points = model.fit_transform(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [entry[0] for entry in counts.most_common(VOCAB_SIZE-1)] + [\"UNK\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.plot(points[:100,0], points[:100,1], 'bo', markersize=0)\n",
    "\n",
    "for i, txt in enumerate(words[:100]):\n",
    "    ax.annotate(txt, (points[i][0],points[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Doc2Vec algorithm\n",
    "class DocEmbeddingGraph(object):\n",
    "    def __init__(self, data_table, batch_size, vocabulary_size, doc_embedding_size,\n",
    "                 word_embedding_size, context_size, train_size, test_size,\n",
    "                 word_embeddings=None, nce_weights=None, nce_biases=None, use_test=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        random.seed(9812)\n",
    "        indices = random.sample(range(len(data_table)), train_size + test_size)\n",
    "        if not use_test:\n",
    "            self.train_indices = indices[:train_size]\n",
    "            self.test_indices = indices[-test_size:]\n",
    "        else:\n",
    "            self.train_indices = indices[-test_size:]\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default():    \n",
    "            self.doc_embeddings = tf.Variable(\n",
    "                tf.random_uniform([len(self.train_indices), doc_embedding_size], -1.0, 1.0),\n",
    "                name=\"doc_embeddings\")\n",
    "\n",
    "            if word_embeddings is None:\n",
    "                self.word_embeddings = tf.Variable(\n",
    "                    tf.random_uniform([vocabulary_size, word_embedding_size], -1.0, 1.0),\n",
    "                    name=\"word_embeddings\")\n",
    "            else:\n",
    "                self.word_embeddings = tf.constant(word_embeddings, name=\"word_embeddings\")\n",
    "\n",
    "            all_embedding_size = doc_embedding_size + word_embedding_size\n",
    "\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(self.doc_embeddings), 1, keep_dims=True))\n",
    "            self.normalized_doc_embeddings = self.doc_embeddings / norm\n",
    "\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(self.word_embeddings), 1, keep_dims=True))\n",
    "            self.normalized_word_embeddings = self.word_embeddings / norm\n",
    "\n",
    "            if nce_weights is None:\n",
    "                self.nce_weights = tf.Variable(\n",
    "                  tf.truncated_normal([vocabulary_size, all_embedding_size],\n",
    "                                      stddev=1.0 / math.sqrt(all_embedding_size)))\n",
    "            else:\n",
    "                self.nce_weights = tf.constant(nce_weights)\n",
    "            \n",
    "            if nce_biases is None:\n",
    "                self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "            else:\n",
    "                self.nce_biases = tf.constant(nce_biases)\n",
    "\n",
    "            self.x_ph = tf.placeholder(tf.int32, shape=[batch_size,CONTEXT_SIZE])\n",
    "            self.doc_ph = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "            self.y_ph = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "\n",
    "            doc_embed = tf.nn.embedding_lookup(self.doc_embeddings, self.doc_ph)\n",
    "            words_embed = tf.add_n(\n",
    "                [tf.nn.embedding_lookup(self.word_embeddings, self.x_ph[:,idx]) for idx in xrange(CONTEXT_SIZE)])\n",
    "\n",
    "            embed = tf.concat(1, [doc_embed, words_embed])\n",
    "\n",
    "            # Compute the NCE loss, using a sample of the negative labels each time.\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(self.nce_weights, self.nce_biases, embed, self.y_ph,\n",
    "                               64, vocabulary_size))\n",
    "\n",
    "            output = tf.transpose(tf.matmul(self.nce_weights, tf.transpose(embed))) + self.nce_biases\n",
    "            self.y_pred = tf.argmax(tf.nn.softmax(output), 1)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "            \n",
    "    def generate_batch(self, indices, sample_ratio):\n",
    "        x = np.ndarray(shape=(self.batch_size, self.context_size), dtype=np.int32)\n",
    "        di = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        y = np.ndarray(shape=(self.batch_size,1), dtype=np.int32)\n",
    "        batch_idx = 0\n",
    "        for doc_idx, index in enumerate(indices):\n",
    "            with open(\"%s/scripts/%s.js\" % (REPO_ROOT, data_table[index][\"sha\"])) as f:\n",
    "                js = f.read()\n",
    "                tokens = [numerize(token) for token in util.tokenize_js(js)]\n",
    "                if len(tokens) < self.context_size + 1:\n",
    "                    continue\n",
    "\n",
    "                sample_size = len(tokens) - 1 - self.context_size\n",
    "                num_to_sample = max(1, int(math.floor(sample_size * sample_ratio)))\n",
    "                for pos in random.sample(range(0, sample_size + 1), num_to_sample):\n",
    "                    di[batch_idx] = doc_idx\n",
    "                    for idx in xrange(self.context_size):\n",
    "                        x[batch_idx,idx] = tokens[pos+idx]\n",
    "                    y[batch_idx,0] = tokens[pos+self.context_size]\n",
    "                    batch_idx += 1\n",
    "                    if batch_idx == self.batch_size:\n",
    "                        yield {self.x_ph: x, self.doc_ph: di, self.y_ph: y}\n",
    "                        batch_idx = 0\n",
    "                        \n",
    "    def train(self, session):\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        batch_idx = 1\n",
    "        average_loss = 0\n",
    "        for feed_dict in self.generate_batch(self.train_indices, 1.0):\n",
    "            _, cur_loss = session.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += cur_loss\n",
    "\n",
    "            if batch_idx % 2000 == 0:\n",
    "                print \"%d Loss: %f\" % (batch_idx, average_loss / 1999)\n",
    "                average_loss = 0\n",
    "\n",
    "            batch_idx += 1                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Training size 300\n"
     ]
    }
   ],
   "source": [
    "model_data = {}\n",
    "\n",
    "for train_size in TRAIN_SIZES:\n",
    "    print \"Doc2Vec Training size %d\" % train_size\n",
    "    \n",
    "    g = DocEmbeddingGraph(\n",
    "        data_table, BATCH_SIZE, VOCAB_SIZE, DOC_EMBEDDING_SIZE, WORD_EMBEDDING_SIZE, CONTEXT_SIZE,\n",
    "        train_size, TEST_SIZE)\n",
    "\n",
    "    with tf.Session(graph=g.graph) as session:\n",
    "        g.train(session)\n",
    "\n",
    "        model_data[\"X_train\"] = g.doc_embeddings.eval()\n",
    "        model_data[\"Y_train\"] = np.array([data_table[index][\"flag-any\"] for index in g.train_indices])\n",
    "\n",
    "        word_embeddings = g.word_embeddings.eval()\n",
    "        nce_weights = g.nce_weights.eval()\n",
    "        nce_biases = g.nce_biases.eval()\n",
    "\n",
    "        print \"Done training. Embeddings: %s, %s, NCE weights %s, %s\" % (\n",
    "            np.shape(model_data[\"X_train\"]), np.shape(word_embeddings),\n",
    "            np.shape(nce_weights), np.shape(nce_biases))\n",
    "        \n",
    "        save_training_set(g.normalized_word_embeddings.eval(), data_table, g.train_indices, g.test_indices)\n",
    "\n",
    "    g2 = DocEmbeddingGraph(\n",
    "        data_table, BATCH_SIZE, VOCAB_SIZE, DOC_EMBEDDING_SIZE, WORD_EMBEDDING_SIZE, CONTEXT_SIZE,\n",
    "        train_size, TEST_SIZE, word_embeddings, nce_weights, nce_biases, True)\n",
    "\n",
    "    with tf.Session(graph=g2.graph) as session:\n",
    "        g2.train(session)\n",
    "\n",
    "        model_data[\"X_test\"] = g2.doc_embeddings.eval()\n",
    "        model_data[\"Y_test\"] = np.array([data_table[index][\"flag-any\"] for index in g2.train_indices])\n",
    "\n",
    "        print \"Done projecting test set. %s\" % (np.shape(model_data[\"X_test\"]),)\n",
    "\n",
    "    with open(\"%s/model-data/dataset_Doc2Vec_%d.pickle\" % (REPO_ROOT, train_size), \"w\") as f:\n",
    "        pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
